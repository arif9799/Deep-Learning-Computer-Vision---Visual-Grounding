{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### Importing Necessary Libraries for Data Preparation & Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:03:37.012714Z",
     "iopub.status.busy": "2023-04-25T03:03:37.012307Z",
     "iopub.status.idle": "2023-04-25T03:03:54.379461Z",
     "shell.execute_reply": "2023-04-25T03:03:54.378109Z",
     "shell.execute_reply.started": "2023-04-25T03:03:37.012668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imagesize\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: imagesize\n",
      "Successfully installed imagesize-1.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "######################################################################################################################\n",
    "#                               Import Python Files for Sentence & Annotations Extraction                            #\n",
    "#                                        Provided as a Simple API on Github                                          #\n",
    "#                                https://github.com/BryanPlummer/flickr30k_entities                                  #\n",
    "######################################################################################################################\n",
    "\n",
    "# Uncomment the below line once in order for the code to run smoothly\n",
    "!pip install imagesize\n",
    "\n",
    "import os\n",
    "os.chdir(r\"/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/\")\n",
    "\n",
    "import Utils.flickr30k_entities_utils\n",
    "from Utils.flickr30k_entities_utils import get_sentence_data, get_annotations\n",
    "\n",
    "import Utils.helper_functions\n",
    "from Utils.helper_functions import *\n",
    "\n",
    "import random \n",
    "import seaborn as sb\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import ast\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.ops import box_iou,generalized_box_iou_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:03:54.382666Z",
     "iopub.status.busy": "2023-04-25T03:03:54.382050Z",
     "iopub.status.idle": "2023-04-25T03:03:54.396491Z",
     "shell.execute_reply": "2023-04-25T03:03:54.394567Z",
     "shell.execute_reply.started": "2023-04-25T03:03:54.382630Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#                                                   Mapping Function                                                 #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\"\"\"\n",
    "Mapping Function does the following,\n",
    "    - takes list of Image names as i/p and fetch Sentences & Annotations (contains bounding boxes)of all those Images\n",
    "    - passes those Sentences & Annotations to the func Phrase_Id_to_Bbox & gets Bounding Boxes for all phrases\n",
    "    in every image.\n",
    "    - also, passes those Sentences & Annotations to the func Phrase_Id_to_Phrases & extracts phrases contained in all\n",
    "    images.\n",
    "    \n",
    "    A typical look of the outputs would be:\n",
    "    \n",
    "        _Image_Train_Phrase_Id_to_Bbox -----> {'image_id_1' : {'Phrase_id_1' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_2' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_3' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Bbox1, Bbox2 ... Bboxn]}\n",
    "                                                              \n",
    "                                                'image_id_2' : {'Phrase_id_1' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_2' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_3' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Bbox1, Bbox2 ... Bboxn]}\n",
    "                                                              \n",
    "                                                              \n",
    "                                                              \n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              \n",
    "                                                              \n",
    "                                                'image_id_n' : {'Phrase_id_1' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_2' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              'Phrase_id_3' : [Bbox1, Bbox2 ... Bboxn],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Bbox1, Bbox2 ... Bboxn]}\n",
    "                                                              \n",
    "                                                              }\n",
    "                                                              \n",
    "                                                              \n",
    "        _Image_Train_Phrase_Id_to_Phrase -----> {'image_id_1' : {'Phrase_id_1' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_2' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_3' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Phrase1, Phrase2.... Phrase_n]}\n",
    "                                                              \n",
    "                                                'image_id_2' : {'Phrase_id_1' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_2' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_3' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Phrase1, Phrase2.... Phrase_n]}\n",
    "                                                              \n",
    "                                                              \n",
    "                                                              \n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              \n",
    "                                                              \n",
    "                                                'image_id_n' : {'Phrase_id_1' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_2' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              'Phrase_id_3' : [Phrase1, Phrase2.... Phrase_n],\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              .\n",
    "                                                              'Phrase_id_n' : [Phrase1, Phrase2.... Phrase_n]}\n",
    "                                                              \n",
    "                                                              }\n",
    "        \n",
    "\n",
    "NOTE: Please alter any folder paths for Images, Sentences and Annotations (Phrase & Bounding Boxes) in Helper Function File\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "def Mapping(_Image_Names, _paths_dict):\n",
    "    _Phrase_Id_to_Bbox = defaultdict()\n",
    "    _Phrase_Id_to_Phrase = defaultdict()\n",
    "\n",
    "    for _img in tqdm(_Image_Names):\n",
    "        _img_sentences_path, _img_annotations_path, _img_absolute_path = get_Paths(_img, _paths_dict)\n",
    "        sents = get_sentence_data(_img_sentences_path)\n",
    "        anns = get_annotations(_img_annotations_path)\n",
    "        _Phrase_Id_to_Bbox[_img] = phrase_Id_to_Bbox(sents, anns)\n",
    "        _Phrase_Id_to_Phrase[_img] = phrase_Id_to_Phrases(sents, anns)\n",
    "        \n",
    "        \n",
    "    return _Phrase_Id_to_Bbox, _Phrase_Id_to_Phrase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:03:54.398934Z",
     "iopub.status.busy": "2023-04-25T03:03:54.398434Z",
     "iopub.status.idle": "2023-04-25T03:03:54.419968Z",
     "shell.execute_reply": "2023-04-25T03:03:54.418315Z",
     "shell.execute_reply.started": "2023-04-25T03:03:54.398893Z"
    }
   },
   "outputs": [],
   "source": [
    "_paths_dict = {\n",
    "                '_sentences_path' : r'/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/Data/annotations/Sentences',\n",
    "                '_annotations_path' : r'/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/Data/annotations/Annotations',\n",
    "                '_image_folder_path' : r'/kaggle/input/flickr30k/flickr30k_images'\n",
    "                }\n",
    "_train_len = 5000 #len(_trainimg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:03:54.425394Z",
     "iopub.status.busy": "2023-04-25T03:03:54.424209Z",
     "iopub.status.idle": "2023-04-25T03:03:54.481183Z",
     "shell.execute_reply": "2023-04-25T03:03:54.479815Z",
     "shell.execute_reply.started": "2023-04-25T03:03:54.425346Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#                       Enter path for train, val & test split in their respective variables                         #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "train.txt, val.txt and test.txt are text files that contains predefined splits, i.e each file contains the split it\n",
    "belongs to.\n",
    "\n",
    "train.txt contains all image names as strings, that should be used for training\n",
    "val.txt contains all image names as strings, that should be used for validation\n",
    "test.txt contains all image names as strings, that should be used for testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "_trainimg = load_Splits('/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/Data/Splits/train.txt')\n",
    "_vlimg = load_Splits('/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/Data/Splits/val.txt')\n",
    "_tsimg = load_Splits('/kaggle/input/vgrutils/Visual Grounding RefEx/Flickr30/Data/Splits/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:03:54.483268Z",
     "iopub.status.busy": "2023-04-25T03:03:54.482934Z",
     "iopub.status.idle": "2023-04-25T03:05:24.905251Z",
     "shell.execute_reply": "2023-04-25T03:05:24.903760Z",
     "shell.execute_reply.started": "2023-04-25T03:03:54.483238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c197772736e4988b311a4a6f637cc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069c122dece4b008249631e41bdfd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce2b9c2352a443ea8085a1c48f6e3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#                                                 Call to the Mapping Functions                                      #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "_fractional_trainimg = _trainimg[:_train_len]\n",
    "_Image_Train_Phrase_Id_to_Bbox, _Image_Train_Phrase_Id_to_Phrase = Mapping(_fractional_trainimg, _paths_dict)\n",
    "_Image_Val_Phrase_Id_to_Bbox, _Image_Val_Phrase_Id_to_Phrase = Mapping(_vlimg, _paths_dict)\n",
    "_Image_Test_Phrase_Id_to_Bbox, _Image_Test_Phrase_Id_to_Phrase = Mapping(_tsimg, _paths_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:05:24.907512Z",
     "iopub.status.busy": "2023-04-25T03:05:24.907036Z",
     "iopub.status.idle": "2023-04-25T03:05:24.925922Z",
     "shell.execute_reply": "2023-04-25T03:05:24.924574Z",
     "shell.execute_reply.started": "2023-04-25T03:05:24.907455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***************************************************************************************************************'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def prepare_DataFrame(Phrase_Dict, Bbox_Dict):\n",
    "    Final_DF = pd.DataFrame()\n",
    "    for Image_Id in tqdm(Phrase_Dict.keys()):\n",
    "        \n",
    "        Phrase_DF = pd.DataFrame.from_dict(Phrase_Dict[Image_Id], orient = 'index')\n",
    "        Phrase_DF = pd.DataFrame(Phrase_DF.stack(level=0)).reset_index().drop('level_1', axis = 1)\n",
    "\n",
    "        Bbox_DF = pd.DataFrame.from_dict(Bbox_Dict[Image_Id], orient = 'index')\n",
    "        Bbox_DF = pd.DataFrame(Bbox_DF.stack(level=0)).reset_index().drop('level_1', axis = 1)\n",
    "        Bbox_DF = Bbox_DF.groupby(['level_0'])[0].apply(list)\n",
    "        \n",
    "\n",
    "        Merged_DF = pd.merge(Phrase_DF, Bbox_DF, on = 'level_0', how='inner')\n",
    "        Merged_DF['Image_Id'] = Image_Id\n",
    "\n",
    "        Final_DF = pd.concat([Final_DF, Merged_DF], axis = 0)\n",
    "\n",
    "    Final_DF = Final_DF.rename(columns = {'level_0' : 'Phrase_Id', '0_x': 'Phrase', '0_y':'Bounding_Box'})\n",
    "    Final_DF = Final_DF[['Image_Id', 'Phrase_Id', 'Phrase', 'Bounding_Box']]\n",
    "    Final_DF.reset_index(drop = True, inplace = True)\n",
    "    print(\"Local Function Called......\")\n",
    "    return Final_DF\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"***************************************************************************************************************\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:05:24.929209Z",
     "iopub.status.busy": "2023-04-25T03:05:24.928186Z",
     "iopub.status.idle": "2023-04-25T03:06:54.809070Z",
     "shell.execute_reply": "2023-04-25T03:06:54.807806Z",
     "shell.execute_reply.started": "2023-04-25T03:05:24.929157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5b35aea574dac889431286d6d44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Function Called......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9977b165ee49ffa12038a5a2162500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Function Called......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7ce1fc816495fabb19563a5e23304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Function Called......\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#                                                Converting to DataFrames.                                           #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "_Fractional_Train_Set_Pid_to_P = {img : _Image_Train_Phrase_Id_to_Phrase[img] for img in _trainimg[:_train_len]}\n",
    "_Fractional_Train_Set_Pid_to_B = {img : _Image_Train_Phrase_Id_to_Bbox[img] for img in _trainimg[:_train_len]}\n",
    "\n",
    "Train_Frame = prepare_DataFrame(_Fractional_Train_Set_Pid_to_P, _Fractional_Train_Set_Pid_to_B)\n",
    "Test_Frame = prepare_DataFrame(_Image_Test_Phrase_Id_to_Phrase, _Image_Test_Phrase_Id_to_Bbox)\n",
    "Val_Frame = prepare_DataFrame(_Image_Val_Phrase_Id_to_Phrase, _Image_Val_Phrase_Id_to_Bbox)\n",
    "\n",
    "\n",
    "Train_Frame.Phrase = Train_Frame.Phrase.str.lower()\n",
    "Val_Frame.Phrase = Val_Frame.Phrase.str.lower()\n",
    "Test_Frame.Phrase = Test_Frame.Phrase.str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Code for ViT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:54.810747Z",
     "iopub.status.busy": "2023-04-25T03:06:54.810416Z",
     "iopub.status.idle": "2023-04-25T03:06:55.263388Z",
     "shell.execute_reply": "2023-04-25T03:06:55.262239Z",
     "shell.execute_reply.started": "2023-04-25T03:06:54.810703Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#                    image_index corresponding to image_id is the index of its embedding                             #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "_v_train_Image_Indices = pd.DataFrame(_trainimg[:_train_len], columns = ['Image_Id']).reset_index().rename(columns = {'index':'image_index'})\n",
    "_v_val_Image_Indices = pd.DataFrame(_vlimg[:_train_len], columns = ['Image_Id']).reset_index().rename(columns = {'index':'image_index'})\n",
    "_v_test_Image_Indices = pd.DataFrame(_tsimg[:_train_len], columns = ['Image_Id']).reset_index().rename(columns = {'index':'image_index'})\n",
    "\n",
    "\n",
    "Train_Frame = Train_Frame.merge(_v_train_Image_Indices, on = 'Image_Id', how='left')\n",
    "Val_Frame = Val_Frame.merge(_v_val_Image_Indices, on = 'Image_Id', how='left')\n",
    "Test_Frame = Test_Frame.merge(_v_test_Image_Indices, on = 'Image_Id', how='left')\n",
    "\n",
    "Vision_Embeddings_train = torch.load('/kaggle/input/embeddings-7k/v_train_embeds.pt')\n",
    "Vision_Embeddings_val = torch.load('/kaggle/input/embeddings-7k/v_val_embeds.pt')\n",
    "Vision_Embeddings_test = torch.load('/kaggle/input/embeddings-7k/v_test_embeds.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Code for BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:55.265675Z",
     "iopub.status.busy": "2023-04-25T03:06:55.265168Z",
     "iopub.status.idle": "2023-04-25T03:06:56.759366Z",
     "shell.execute_reply": "2023-04-25T03:06:56.757992Z",
     "shell.execute_reply.started": "2023-04-25T03:06:55.265623Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "######################################################################################################################\n",
    "#                                                                                                                    #\n",
    "#               image_index corresponding to unique Phrase is the index of its embedding                             #\n",
    "#                                                                                                                    #\n",
    "######################################################################################################################\n",
    "\"\"\"\n",
    "with open('/kaggle/input/embeddings-7k/_train_Phrase_to_Index_Map.pkl', 'rb') as fp:\n",
    "    _train_Phrase_to_Index_Map = pickle.load(fp)\n",
    "\n",
    "with open('/kaggle/input/embeddings-7k/_val_Phrase_to_Index_Map.pkl', 'rb') as fp:\n",
    "    _val_Phrase_to_Index_Map = pickle.load(fp)\n",
    "    \n",
    "with open('/kaggle/input/embeddings-7k/_test_Phrase_to_Index_Map.pkl', 'rb') as fp:\n",
    "    _test_Phrase_to_Index_Map = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "_t_train_Image_Indices = pd.DataFrame(_train_Phrase_to_Index_Map.items(), columns = ['Phrase', 'text_index'])\n",
    "_t_val_Image_Indices = pd.DataFrame(_val_Phrase_to_Index_Map.items(), columns = ['Phrase', 'text_index'])\n",
    "_t_test_Image_Indices = pd.DataFrame(_test_Phrase_to_Index_Map.items(), columns = ['Phrase', 'text_index'])\n",
    "\n",
    "Train_Frame = Train_Frame.merge(_t_train_Image_Indices, on = 'Phrase', how = 'left')\n",
    "Val_Frame = Val_Frame.merge(_t_val_Image_Indices, on = 'Phrase', how = 'left')\n",
    "Test_Frame = Test_Frame.merge(_t_test_Image_Indices, on = 'Phrase', how = 'left')\n",
    "\n",
    "\n",
    "Textual_Embeddings_train = torch.load('/kaggle/input/embeddings-7k/t_train_embeds.pt')\n",
    "Textual_Embeddings_val = torch.load('/kaggle/input/embeddings-7k/t_val_embeds.pt')\n",
    "Textual_Embeddings_test = torch.load('/kaggle/input/embeddings-7k/t_test_embeds.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Dataframes Look Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.765656Z",
     "iopub.status.busy": "2023-04-25T03:06:56.765228Z",
     "iopub.status.idle": "2023-04-25T03:06:56.782052Z",
     "shell.execute_reply": "2023-04-25T03:06:56.780822Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.765615Z"
    }
   },
   "outputs": [],
   "source": [
    "just_to_see = ['Image_Id', 'Phrase_Id', 'Phrase']\n",
    "necessary_columns = ['image_index', 'text_index', 'Bounding_Box']\n",
    "train = Train_Frame[just_to_see + necessary_columns] \n",
    "val = Val_Frame[just_to_see + necessary_columns] \n",
    "test = Test_Frame[just_to_see + necessary_columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.784603Z",
     "iopub.status.busy": "2023-04-25T03:06:56.783543Z",
     "iopub.status.idle": "2023-04-25T03:06:56.872903Z",
     "shell.execute_reply": "2023-04-25T03:06:56.871904Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.784546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Id</th>\n",
       "      <th>Phrase_Id</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>image_index</th>\n",
       "      <th>text_index</th>\n",
       "      <th>Bounding_Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112630</td>\n",
       "      <td>two people</td>\n",
       "      <td>0</td>\n",
       "      <td>1657</td>\n",
       "      <td>[[46, 182, 105, 333], [143, 165, 207, 333]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112632</td>\n",
       "      <td>the video game shop</td>\n",
       "      <td>0</td>\n",
       "      <td>15491</td>\n",
       "      <td>[[0, 54, 168, 307]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112631</td>\n",
       "      <td>the mobile phone store</td>\n",
       "      <td>0</td>\n",
       "      <td>15492</td>\n",
       "      <td>[[191, 0, 498, 230]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112625</td>\n",
       "      <td>people</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[46, 182, 105, 333], [143, 165, 207, 333], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112625</td>\n",
       "      <td>a group of people</td>\n",
       "      <td>0</td>\n",
       "      <td>15493</td>\n",
       "      <td>[[46, 182, 105, 333], [143, 165, 207, 333], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112625</td>\n",
       "      <td>several people</td>\n",
       "      <td>0</td>\n",
       "      <td>1658</td>\n",
       "      <td>[[46, 182, 105, 333], [143, 165, 207, 333], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112627</td>\n",
       "      <td>some stores</td>\n",
       "      <td>0</td>\n",
       "      <td>1659</td>\n",
       "      <td>[[191, 0, 498, 230], [1, 0, 190, 307]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3359636318</td>\n",
       "      <td>112626</td>\n",
       "      <td>a sidewalk</td>\n",
       "      <td>0</td>\n",
       "      <td>1660</td>\n",
       "      <td>[[2, 212, 499, 333]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262504</td>\n",
       "      <td>many people of all races</td>\n",
       "      <td>1</td>\n",
       "      <td>18720</td>\n",
       "      <td>[[5, 70, 103, 314], [120, 54, 206, 172], [197,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262504</td>\n",
       "      <td>a series of spectators</td>\n",
       "      <td>1</td>\n",
       "      <td>15494</td>\n",
       "      <td>[[5, 70, 103, 314], [120, 54, 206, 172], [197,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262504</td>\n",
       "      <td>a small crowd</td>\n",
       "      <td>1</td>\n",
       "      <td>8125</td>\n",
       "      <td>[[5, 70, 103, 314], [120, 54, 206, 172], [197,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262504</td>\n",
       "      <td>the crowd</td>\n",
       "      <td>1</td>\n",
       "      <td>1661</td>\n",
       "      <td>[[5, 70, 103, 314], [120, 54, 206, 172], [197,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262509</td>\n",
       "      <td>red , white , and black balloons</td>\n",
       "      <td>1</td>\n",
       "      <td>20867</td>\n",
       "      <td>[[249, 123, 340, 213]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262505</td>\n",
       "      <td>two men</td>\n",
       "      <td>1</td>\n",
       "      <td>1662</td>\n",
       "      <td>[[5, 70, 103, 314], [120, 54, 206, 172]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262503</td>\n",
       "      <td>a man</td>\n",
       "      <td>1</td>\n",
       "      <td>1663</td>\n",
       "      <td>[[5, 70, 103, 314]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262503</td>\n",
       "      <td>runners</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[5, 70, 103, 314]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262503</td>\n",
       "      <td>runner</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[[5, 70, 103, 314]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262514</td>\n",
       "      <td>a tattoo</td>\n",
       "      <td>1</td>\n",
       "      <td>1664</td>\n",
       "      <td>[[19, 150, 47, 197]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262515</td>\n",
       "      <td>headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[60, 100, 85, 200], [59, 105, 87, 163]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6959556104</td>\n",
       "      <td>262511</td>\n",
       "      <td>runs</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[6, 71, 103, 281]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>several people</td>\n",
       "      <td>2</td>\n",
       "      <td>1658</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>commuters</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>pedestrians</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>people</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41841</td>\n",
       "      <td>many people</td>\n",
       "      <td>2</td>\n",
       "      <td>1665</td>\n",
       "      <td>[[13, 32, 137, 332], [212, 57, 320, 331], [333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41842</td>\n",
       "      <td>clothing styles</td>\n",
       "      <td>2</td>\n",
       "      <td>1666</td>\n",
       "      <td>[[215, 95, 315, 329], [13, 69, 141, 332], [404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41843</td>\n",
       "      <td>a sidewalk</td>\n",
       "      <td>2</td>\n",
       "      <td>1660</td>\n",
       "      <td>[[0, 205, 430, 291]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41844</td>\n",
       "      <td>the curb</td>\n",
       "      <td>2</td>\n",
       "      <td>1667</td>\n",
       "      <td>[[4, 233, 499, 313]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2326294409</td>\n",
       "      <td>41845</td>\n",
       "      <td>a crosswalk</td>\n",
       "      <td>2</td>\n",
       "      <td>1668</td>\n",
       "      <td>[[1, 274, 499, 332]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image_Id Phrase_Id                            Phrase  image_index  \\\n",
       "0   3359636318    112630                        two people            0   \n",
       "1   3359636318    112632               the video game shop            0   \n",
       "2   3359636318    112631            the mobile phone store            0   \n",
       "3   3359636318    112625                            people            0   \n",
       "4   3359636318    112625                 a group of people            0   \n",
       "5   3359636318    112625                    several people            0   \n",
       "6   3359636318    112627                       some stores            0   \n",
       "7   3359636318    112626                        a sidewalk            0   \n",
       "8   6959556104    262504          many people of all races            1   \n",
       "9   6959556104    262504            a series of spectators            1   \n",
       "10  6959556104    262504                     a small crowd            1   \n",
       "11  6959556104    262504                         the crowd            1   \n",
       "12  6959556104    262509  red , white , and black balloons            1   \n",
       "13  6959556104    262505                           two men            1   \n",
       "14  6959556104    262503                             a man            1   \n",
       "15  6959556104    262503                           runners            1   \n",
       "16  6959556104    262503                            runner            1   \n",
       "17  6959556104    262514                          a tattoo            1   \n",
       "18  6959556104    262515                        headphones            1   \n",
       "19  6959556104    262511                              runs            1   \n",
       "20  2326294409     41841                    several people            2   \n",
       "21  2326294409     41841                               all            2   \n",
       "22  2326294409     41841                         commuters            2   \n",
       "23  2326294409     41841                       pedestrians            2   \n",
       "24  2326294409     41841                            people            2   \n",
       "25  2326294409     41841                       many people            2   \n",
       "26  2326294409     41842                   clothing styles            2   \n",
       "27  2326294409     41843                        a sidewalk            2   \n",
       "28  2326294409     41844                          the curb            2   \n",
       "29  2326294409     41845                       a crosswalk            2   \n",
       "\n",
       "    text_index                                       Bounding_Box  \n",
       "0         1657        [[46, 182, 105, 333], [143, 165, 207, 333]]  \n",
       "1        15491                                [[0, 54, 168, 307]]  \n",
       "2        15492                               [[191, 0, 498, 230]]  \n",
       "3            0  [[46, 182, 105, 333], [143, 165, 207, 333], [2...  \n",
       "4        15493  [[46, 182, 105, 333], [143, 165, 207, 333], [2...  \n",
       "5         1658  [[46, 182, 105, 333], [143, 165, 207, 333], [2...  \n",
       "6         1659             [[191, 0, 498, 230], [1, 0, 190, 307]]  \n",
       "7         1660                               [[2, 212, 499, 333]]  \n",
       "8        18720  [[5, 70, 103, 314], [120, 54, 206, 172], [197,...  \n",
       "9        15494  [[5, 70, 103, 314], [120, 54, 206, 172], [197,...  \n",
       "10        8125  [[5, 70, 103, 314], [120, 54, 206, 172], [197,...  \n",
       "11        1661  [[5, 70, 103, 314], [120, 54, 206, 172], [197,...  \n",
       "12       20867                             [[249, 123, 340, 213]]  \n",
       "13        1662           [[5, 70, 103, 314], [120, 54, 206, 172]]  \n",
       "14        1663                                [[5, 70, 103, 314]]  \n",
       "15           1                                [[5, 70, 103, 314]]  \n",
       "16           2                                [[5, 70, 103, 314]]  \n",
       "17        1664                               [[19, 150, 47, 197]]  \n",
       "18           3           [[60, 100, 85, 200], [59, 105, 87, 163]]  \n",
       "19           4                                [[6, 71, 103, 281]]  \n",
       "20        1658  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "21           5  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "22           6  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "23           7  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "24           0  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "25        1665  [[13, 32, 137, 332], [212, 57, 320, 331], [333...  \n",
       "26        1666  [[215, 95, 315, 329], [13, 69, 141, 332], [404...  \n",
       "27        1660                               [[0, 205, 430, 291]]  \n",
       "28        1667                               [[4, 233, 499, 313]]  \n",
       "29        1668                               [[1, 274, 499, 332]]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.876028Z",
     "iopub.status.busy": "2023-04-25T03:06:56.874419Z",
     "iopub.status.idle": "2023-04-25T03:06:56.913770Z",
     "shell.execute_reply": "2023-04-25T03:06:56.912793Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.875971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Id</th>\n",
       "      <th>Phrase_Id</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>image_index</th>\n",
       "      <th>text_index</th>\n",
       "      <th>Bounding_Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100652400</td>\n",
       "      <td>197</td>\n",
       "      <td>a man</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>[[52, 44, 109, 202]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100652400</td>\n",
       "      <td>197</td>\n",
       "      <td>a construction worker</td>\n",
       "      <td>0</td>\n",
       "      <td>2825</td>\n",
       "      <td>[[52, 44, 109, 202]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100652400</td>\n",
       "      <td>198</td>\n",
       "      <td>a blue hard hat</td>\n",
       "      <td>0</td>\n",
       "      <td>4767</td>\n",
       "      <td>[[58, 43, 87, 65]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100652400</td>\n",
       "      <td>198</td>\n",
       "      <td>hard hat</td>\n",
       "      <td>0</td>\n",
       "      <td>617</td>\n",
       "      <td>[[58, 43, 87, 65]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100652400</td>\n",
       "      <td>198</td>\n",
       "      <td>a hard hat</td>\n",
       "      <td>0</td>\n",
       "      <td>2826</td>\n",
       "      <td>[[58, 43, 87, 65]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100652400</td>\n",
       "      <td>199</td>\n",
       "      <td>a reflective vest</td>\n",
       "      <td>0</td>\n",
       "      <td>2828</td>\n",
       "      <td>[[61, 68, 97, 118]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100652400</td>\n",
       "      <td>199</td>\n",
       "      <td>orange safety vest</td>\n",
       "      <td>0</td>\n",
       "      <td>2827</td>\n",
       "      <td>[[61, 68, 97, 118]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100652400</td>\n",
       "      <td>199</td>\n",
       "      <td>bright vest</td>\n",
       "      <td>0</td>\n",
       "      <td>618</td>\n",
       "      <td>[[61, 68, 97, 118]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100652400</td>\n",
       "      <td>199</td>\n",
       "      <td>a caution vest</td>\n",
       "      <td>0</td>\n",
       "      <td>2829</td>\n",
       "      <td>[[61, 68, 97, 118]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100652400</td>\n",
       "      <td>200</td>\n",
       "      <td>an intersection</td>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "      <td>[[0, 89, 373, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100652400</td>\n",
       "      <td>200</td>\n",
       "      <td>a street corner</td>\n",
       "      <td>0</td>\n",
       "      <td>2830</td>\n",
       "      <td>[[0, 89, 373, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100652400</td>\n",
       "      <td>200</td>\n",
       "      <td>the street</td>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>[[0, 89, 373, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100652400</td>\n",
       "      <td>200</td>\n",
       "      <td>the road</td>\n",
       "      <td>0</td>\n",
       "      <td>621</td>\n",
       "      <td>[[0, 89, 373, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100652400</td>\n",
       "      <td>201</td>\n",
       "      <td>an orange flag</td>\n",
       "      <td>0</td>\n",
       "      <td>2832</td>\n",
       "      <td>[[92, 139, 159, 183]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100652400</td>\n",
       "      <td>201</td>\n",
       "      <td>a flag</td>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "      <td>[[92, 139, 159, 183]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100652400</td>\n",
       "      <td>201</td>\n",
       "      <td>a red flag</td>\n",
       "      <td>0</td>\n",
       "      <td>2831</td>\n",
       "      <td>[[92, 139, 159, 183]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100652400</td>\n",
       "      <td>202</td>\n",
       "      <td>spray paint</td>\n",
       "      <td>0</td>\n",
       "      <td>623</td>\n",
       "      <td>[[320, 146, 374, 206], [103, 173, 225, 249], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>a pink cart</td>\n",
       "      <td>1</td>\n",
       "      <td>2833</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>the man</td>\n",
       "      <td>1</td>\n",
       "      <td>624</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>a stroller</td>\n",
       "      <td>1</td>\n",
       "      <td>626</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>their cart</td>\n",
       "      <td>1</td>\n",
       "      <td>625</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>a man</td>\n",
       "      <td>1</td>\n",
       "      <td>616</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>101958970</td>\n",
       "      <td>597</td>\n",
       "      <td>his daughter 's pink car</td>\n",
       "      <td>1</td>\n",
       "      <td>5503</td>\n",
       "      <td>[[74, 60, 397, 373]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>101958970</td>\n",
       "      <td>596</td>\n",
       "      <td>the little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>2835</td>\n",
       "      <td>[[237, 192, 356, 368]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>101958970</td>\n",
       "      <td>596</td>\n",
       "      <td>his daughter</td>\n",
       "      <td>1</td>\n",
       "      <td>627</td>\n",
       "      <td>[[237, 192, 356, 368]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>101958970</td>\n",
       "      <td>596</td>\n",
       "      <td>a little girl</td>\n",
       "      <td>1</td>\n",
       "      <td>2834</td>\n",
       "      <td>[[237, 192, 356, 368]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>102851549</td>\n",
       "      <td>798</td>\n",
       "      <td>a little boy</td>\n",
       "      <td>2</td>\n",
       "      <td>2836</td>\n",
       "      <td>[[73, 258, 191, 476]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>102851549</td>\n",
       "      <td>798</td>\n",
       "      <td>boy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[73, 258, 191, 476]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>102851549</td>\n",
       "      <td>798</td>\n",
       "      <td>a child</td>\n",
       "      <td>2</td>\n",
       "      <td>628</td>\n",
       "      <td>[[73, 258, 191, 476]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>102851549</td>\n",
       "      <td>798</td>\n",
       "      <td>a little boy</td>\n",
       "      <td>2</td>\n",
       "      <td>2836</td>\n",
       "      <td>[[73, 258, 191, 476]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Image_Id Phrase_Id                    Phrase  image_index  text_index  \\\n",
       "0   100652400       197                     a man            0         616   \n",
       "1   100652400       197     a construction worker            0        2825   \n",
       "2   100652400       198           a blue hard hat            0        4767   \n",
       "3   100652400       198                  hard hat            0         617   \n",
       "4   100652400       198                a hard hat            0        2826   \n",
       "5   100652400       199         a reflective vest            0        2828   \n",
       "6   100652400       199        orange safety vest            0        2827   \n",
       "7   100652400       199               bright vest            0         618   \n",
       "8   100652400       199            a caution vest            0        2829   \n",
       "9   100652400       200           an intersection            0         619   \n",
       "10  100652400       200           a street corner            0        2830   \n",
       "11  100652400       200                the street            0         620   \n",
       "12  100652400       200                  the road            0         621   \n",
       "13  100652400       201            an orange flag            0        2832   \n",
       "14  100652400       201                    a flag            0         622   \n",
       "15  100652400       201                a red flag            0        2831   \n",
       "16  100652400       202               spray paint            0         623   \n",
       "17  101958970       597               a pink cart            1        2833   \n",
       "18  101958970       597                   the man            1         624   \n",
       "19  101958970       597                a stroller            1         626   \n",
       "20  101958970       597                their cart            1         625   \n",
       "21  101958970       597                     a man            1         616   \n",
       "22  101958970       597  his daughter 's pink car            1        5503   \n",
       "23  101958970       596           the little girl            1        2835   \n",
       "24  101958970       596              his daughter            1         627   \n",
       "25  101958970       596             a little girl            1        2834   \n",
       "26  102851549       798              a little boy            2        2836   \n",
       "27  102851549       798                       boy            2           0   \n",
       "28  102851549       798                   a child            2         628   \n",
       "29  102851549       798              a little boy            2        2836   \n",
       "\n",
       "                                         Bounding_Box  \n",
       "0                                [[52, 44, 109, 202]]  \n",
       "1                                [[52, 44, 109, 202]]  \n",
       "2                                  [[58, 43, 87, 65]]  \n",
       "3                                  [[58, 43, 87, 65]]  \n",
       "4                                  [[58, 43, 87, 65]]  \n",
       "5                                 [[61, 68, 97, 118]]  \n",
       "6                                 [[61, 68, 97, 118]]  \n",
       "7                                 [[61, 68, 97, 118]]  \n",
       "8                                 [[61, 68, 97, 118]]  \n",
       "9                                 [[0, 89, 373, 499]]  \n",
       "10                                [[0, 89, 373, 499]]  \n",
       "11                                [[0, 89, 373, 499]]  \n",
       "12                                [[0, 89, 373, 499]]  \n",
       "13                              [[92, 139, 159, 183]]  \n",
       "14                              [[92, 139, 159, 183]]  \n",
       "15                              [[92, 139, 159, 183]]  \n",
       "16  [[320, 146, 374, 206], [103, 173, 225, 249], [...  \n",
       "17                               [[74, 60, 397, 373]]  \n",
       "18                               [[74, 60, 397, 373]]  \n",
       "19                               [[74, 60, 397, 373]]  \n",
       "20                               [[74, 60, 397, 373]]  \n",
       "21                               [[74, 60, 397, 373]]  \n",
       "22                               [[74, 60, 397, 373]]  \n",
       "23                             [[237, 192, 356, 368]]  \n",
       "24                             [[237, 192, 356, 368]]  \n",
       "25                             [[237, 192, 356, 368]]  \n",
       "26                              [[73, 258, 191, 476]]  \n",
       "27                              [[73, 258, 191, 476]]  \n",
       "28                              [[73, 258, 191, 476]]  \n",
       "29                              [[73, 258, 191, 476]]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.915934Z",
     "iopub.status.busy": "2023-04-25T03:06:56.915346Z",
     "iopub.status.idle": "2023-04-25T03:06:56.966013Z",
     "shell.execute_reply": "2023-04-25T03:06:56.964541Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.915897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Id</th>\n",
       "      <th>Phrase_Id</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>image_index</th>\n",
       "      <th>text_index</th>\n",
       "      <th>Bounding_Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>547</td>\n",
       "      <td>several climbers</td>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>[[193, 369, 230, 453], [207, 303, 255, 383], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>547</td>\n",
       "      <td>a group of people</td>\n",
       "      <td>0</td>\n",
       "      <td>4644</td>\n",
       "      <td>[[193, 369, 230, 453], [207, 303, 255, 383], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>547</td>\n",
       "      <td>seven climbers</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>[[193, 369, 230, 453], [207, 303, 255, 383], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>547</td>\n",
       "      <td>a collage of one person</td>\n",
       "      <td>0</td>\n",
       "      <td>5381</td>\n",
       "      <td>[[193, 369, 230, 453], [207, 303, 255, 383], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>548</td>\n",
       "      <td>a rock climbing wall</td>\n",
       "      <td>0</td>\n",
       "      <td>4645</td>\n",
       "      <td>[[0, 53, 332, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>548</td>\n",
       "      <td>a cliff</td>\n",
       "      <td>0</td>\n",
       "      <td>603</td>\n",
       "      <td>[[0, 53, 332, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>548</td>\n",
       "      <td>a rock face</td>\n",
       "      <td>0</td>\n",
       "      <td>2731</td>\n",
       "      <td>[[0, 53, 332, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>548</td>\n",
       "      <td>the rock</td>\n",
       "      <td>0</td>\n",
       "      <td>601</td>\n",
       "      <td>[[0, 53, 332, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>548</td>\n",
       "      <td>a rock</td>\n",
       "      <td>0</td>\n",
       "      <td>602</td>\n",
       "      <td>[[0, 53, 332, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>549</td>\n",
       "      <td>one man</td>\n",
       "      <td>0</td>\n",
       "      <td>606</td>\n",
       "      <td>[[73, 301, 180, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>549</td>\n",
       "      <td>the man</td>\n",
       "      <td>0</td>\n",
       "      <td>604</td>\n",
       "      <td>[[73, 301, 180, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>549</td>\n",
       "      <td>another man</td>\n",
       "      <td>0</td>\n",
       "      <td>605</td>\n",
       "      <td>[[73, 301, 180, 499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>550</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[79, 377, 141, 434], [74, 326, 124, 381]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>551</td>\n",
       "      <td>the rope</td>\n",
       "      <td>0</td>\n",
       "      <td>607</td>\n",
       "      <td>[[118, 80, 187, 487]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1016887272</td>\n",
       "      <td>551</td>\n",
       "      <td>the line</td>\n",
       "      <td>0</td>\n",
       "      <td>608</td>\n",
       "      <td>[[118, 80, 187, 487]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266245</td>\n",
       "      <td>two young , wet boys</td>\n",
       "      <td>1</td>\n",
       "      <td>5382</td>\n",
       "      <td>[[49, 24, 252, 280], [225, 66, 305, 188]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266245</td>\n",
       "      <td>two brothers</td>\n",
       "      <td>1</td>\n",
       "      <td>610</td>\n",
       "      <td>[[49, 24, 252, 280], [225, 66, 305, 188]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266245</td>\n",
       "      <td>two children</td>\n",
       "      <td>1</td>\n",
       "      <td>609</td>\n",
       "      <td>[[49, 24, 252, 280], [225, 66, 305, 188]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266245</td>\n",
       "      <td>two little boys</td>\n",
       "      <td>1</td>\n",
       "      <td>2732</td>\n",
       "      <td>[[49, 24, 252, 280], [225, 66, 305, 188]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266246</td>\n",
       "      <td>a beach</td>\n",
       "      <td>1</td>\n",
       "      <td>611</td>\n",
       "      <td>[[0, 115, 499, 331], [0, 42, 499, 332]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266246</td>\n",
       "      <td>the beach</td>\n",
       "      <td>1</td>\n",
       "      <td>612</td>\n",
       "      <td>[[0, 115, 499, 331], [0, 42, 499, 332]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266247</td>\n",
       "      <td>the sand</td>\n",
       "      <td>1</td>\n",
       "      <td>613</td>\n",
       "      <td>[[0, 115, 499, 331]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266247</td>\n",
       "      <td>sand</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 115, 499, 331]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7162685234</td>\n",
       "      <td>266248</td>\n",
       "      <td>the ocean</td>\n",
       "      <td>1</td>\n",
       "      <td>614</td>\n",
       "      <td>[[0, 2, 498, 110]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86433</td>\n",
       "      <td>five men</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>[[120, 1, 195, 221], [295, 33, 390, 212], [371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86434</td>\n",
       "      <td>jeans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[311, 115, 349, 205], [121, 100, 172, 216], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86435</td>\n",
       "      <td>this band</td>\n",
       "      <td>2</td>\n",
       "      <td>617</td>\n",
       "      <td>[[124, 2, 423, 220]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86435</td>\n",
       "      <td>a band</td>\n",
       "      <td>2</td>\n",
       "      <td>616</td>\n",
       "      <td>[[124, 2, 423, 220]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86435</td>\n",
       "      <td>a five member band</td>\n",
       "      <td>2</td>\n",
       "      <td>4646</td>\n",
       "      <td>[[124, 2, 423, 220]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3000017878</td>\n",
       "      <td>86435</td>\n",
       "      <td>a band</td>\n",
       "      <td>2</td>\n",
       "      <td>616</td>\n",
       "      <td>[[124, 2, 423, 220]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image_Id Phrase_Id                   Phrase  image_index  text_index  \\\n",
       "0   1016887272       547         several climbers            0         599   \n",
       "1   1016887272       547        a group of people            0        4644   \n",
       "2   1016887272       547           seven climbers            0         600   \n",
       "3   1016887272       547  a collage of one person            0        5381   \n",
       "4   1016887272       548     a rock climbing wall            0        4645   \n",
       "5   1016887272       548                  a cliff            0         603   \n",
       "6   1016887272       548              a rock face            0        2731   \n",
       "7   1016887272       548                 the rock            0         601   \n",
       "8   1016887272       548                   a rock            0         602   \n",
       "9   1016887272       549                  one man            0         606   \n",
       "10  1016887272       549                  the man            0         604   \n",
       "11  1016887272       549              another man            0         605   \n",
       "12  1016887272       550                      red            0           0   \n",
       "13  1016887272       551                 the rope            0         607   \n",
       "14  1016887272       551                 the line            0         608   \n",
       "15  7162685234    266245     two young , wet boys            1        5382   \n",
       "16  7162685234    266245             two brothers            1         610   \n",
       "17  7162685234    266245             two children            1         609   \n",
       "18  7162685234    266245          two little boys            1        2732   \n",
       "19  7162685234    266246                  a beach            1         611   \n",
       "20  7162685234    266246                the beach            1         612   \n",
       "21  7162685234    266247                 the sand            1         613   \n",
       "22  7162685234    266247                     sand            1           1   \n",
       "23  7162685234    266248                the ocean            1         614   \n",
       "24  3000017878     86433                 five men            2         615   \n",
       "25  3000017878     86434                    jeans            2           2   \n",
       "26  3000017878     86435                this band            2         617   \n",
       "27  3000017878     86435                   a band            2         616   \n",
       "28  3000017878     86435       a five member band            2        4646   \n",
       "29  3000017878     86435                   a band            2         616   \n",
       "\n",
       "                                         Bounding_Box  \n",
       "0   [[193, 369, 230, 453], [207, 303, 255, 383], [...  \n",
       "1   [[193, 369, 230, 453], [207, 303, 255, 383], [...  \n",
       "2   [[193, 369, 230, 453], [207, 303, 255, 383], [...  \n",
       "3   [[193, 369, 230, 453], [207, 303, 255, 383], [...  \n",
       "4                                 [[0, 53, 332, 499]]  \n",
       "5                                 [[0, 53, 332, 499]]  \n",
       "6                                 [[0, 53, 332, 499]]  \n",
       "7                                 [[0, 53, 332, 499]]  \n",
       "8                                 [[0, 53, 332, 499]]  \n",
       "9                               [[73, 301, 180, 499]]  \n",
       "10                              [[73, 301, 180, 499]]  \n",
       "11                              [[73, 301, 180, 499]]  \n",
       "12         [[79, 377, 141, 434], [74, 326, 124, 381]]  \n",
       "13                              [[118, 80, 187, 487]]  \n",
       "14                              [[118, 80, 187, 487]]  \n",
       "15          [[49, 24, 252, 280], [225, 66, 305, 188]]  \n",
       "16          [[49, 24, 252, 280], [225, 66, 305, 188]]  \n",
       "17          [[49, 24, 252, 280], [225, 66, 305, 188]]  \n",
       "18          [[49, 24, 252, 280], [225, 66, 305, 188]]  \n",
       "19            [[0, 115, 499, 331], [0, 42, 499, 332]]  \n",
       "20            [[0, 115, 499, 331], [0, 42, 499, 332]]  \n",
       "21                               [[0, 115, 499, 331]]  \n",
       "22                               [[0, 115, 499, 331]]  \n",
       "23                                 [[0, 2, 498, 110]]  \n",
       "24  [[120, 1, 195, 221], [295, 33, 390, 212], [371...  \n",
       "25  [[311, 115, 349, 205], [121, 100, 172, 216], [...  \n",
       "26                               [[124, 2, 423, 220]]  \n",
       "27                               [[124, 2, 423, 220]]  \n",
       "28                               [[124, 2, 423, 220]]  \n",
       "29                               [[124, 2, 423, 220]]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.968393Z",
     "iopub.status.busy": "2023-04-25T03:06:56.967863Z",
     "iopub.status.idle": "2023-04-25T03:06:56.978623Z",
     "shell.execute_reply": "2023-04-25T03:06:56.977175Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.968338Z"
    }
   },
   "outputs": [],
   "source": [
    "num_hid_dims = 0\n",
    "def _the_Collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    #print(batch_size)\n",
    "    image_index_tensor = []\n",
    "    text_index_tensor = []\n",
    "    image_emb_tensor = []\n",
    "    phrase_emb_tensor = []\n",
    "    bbox_tensor = []\n",
    "    \n",
    "    for idx, (im_idx, t_idx, im_emb, p_emb, bbox) in enumerate(batch):\n",
    "        image_index_tensor.append(im_idx)\n",
    "        text_index_tensor.append(t_idx)\n",
    "        image_emb_tensor.append(im_emb)\n",
    "        phrase_emb_tensor.append(p_emb)\n",
    "        bbox_tensor.append(bbox[0])\n",
    "        \n",
    "        \n",
    "    \"\"\"pad = [[0, 0, 0, 0]] * num_hid_dims\n",
    "    for index, _ in enumerate(bbox_tensor):\n",
    "        temp_pad = pad\n",
    "        temp_pad[:len(bbox_tensor[index])] = bbox_tensor[index]\n",
    "        bbox_tensor[index] = torch.tensor(temp_pad)\"\"\"\n",
    "    \n",
    "    image_index_tensor = torch.tensor(image_index_tensor)\n",
    "    text_index_tensor = torch.tensor(text_index_tensor)\n",
    "    image_emb_tensor = torch.stack(image_emb_tensor)\n",
    "    phrase_emb_tensor = torch.stack(phrase_emb_tensor)\n",
    "    bbox_tensor = torch.tensor(bbox_tensor)\n",
    "    #print(bbox_tensor)\n",
    "    \n",
    "    return image_index_tensor, text_index_tensor, image_emb_tensor, phrase_emb_tensor, bbox_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.981270Z",
     "iopub.status.busy": "2023-04-25T03:06:56.980325Z",
     "iopub.status.idle": "2023-04-25T03:06:56.993525Z",
     "shell.execute_reply": "2023-04-25T03:06:56.991718Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.981207Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_emb,text_emb):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_emb = img_emb\n",
    "        self.text_emb = text_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_index = self.dataframe['image_index'][index]\n",
    "        text_index = self.dataframe['text_index'][index]\n",
    "        image_embedding = self.image_emb[image_index]\n",
    "        phrase_embedding = self.text_emb[text_index]\n",
    "        bounding_boxes = self.dataframe['Bounding_Box'][index]\n",
    "        return image_index, text_index, image_embedding, phrase_embedding, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:56.996230Z",
     "iopub.status.busy": "2023-04-25T03:06:56.995655Z",
     "iopub.status.idle": "2023-04-25T03:06:57.012029Z",
     "shell.execute_reply": "2023-04-25T03:06:57.010796Z",
     "shell.execute_reply.started": "2023-04-25T03:06:56.996185Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDataset(train, img_emb=Vision_Embeddings_train, text_emb=Textual_Embeddings_train)\n",
    "val_dataset = CustomDataset(val, img_emb=Vision_Embeddings_val, text_emb=Textual_Embeddings_val)\n",
    "test_dataset = CustomDataset(test,img_emb=Vision_Embeddings_test,text_emb=Textual_Embeddings_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn= _the_Collate,drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn= _the_Collate,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn= _the_Collate,drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.015086Z",
     "iopub.status.busy": "2023-04-25T03:06:57.013722Z",
     "iopub.status.idle": "2023-04-25T03:06:57.027153Z",
     "shell.execute_reply": "2023-04-25T03:06:57.025773Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.015034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for idx, (im_idx, t_idx, im_emb, p_emb, bbox) in enumerate(train_loader):\\n    print(\"Batch number : \", idx)\\n    print(im_idx)\\n    print(t_idx)\\n    print(im_emb)\\n    print(p_emb)\\n    print(bbox.size())\\n    break'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for idx, (im_idx, t_idx, im_emb, p_emb, bbox) in enumerate(train_loader):\n",
    "    print(\"Batch number : \", idx)\n",
    "    print(im_idx)\n",
    "    print(t_idx)\n",
    "    print(im_emb)\n",
    "    print(p_emb)\n",
    "    print(bbox.size())\n",
    "    break\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.029187Z",
     "iopub.status.busy": "2023-04-25T03:06:57.028672Z",
     "iopub.status.idle": "2023-04-25T03:06:57.042292Z",
     "shell.execute_reply": "2023-04-25T03:06:57.040919Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.029137Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that adds positional encoding to each of the token's features.\n",
    "    So that the Transformer is position aware.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, max_len: int=10000):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension about the features for each token\n",
    "        - max_len: The maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the positional encoding and add it to x.\n",
    "        \n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "          \n",
    "        Return:\n",
    "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        input_dim = x.shape[1]\n",
    "        \n",
    "        pe = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the positional encoding                                   #\n",
    "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "        #                                                                         #\n",
    "        # It's a bit messy, but the definition is provided for your here for your #\n",
    "        # convenience (in LaTex).                                                 #\n",
    "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel})                           #\n",
    "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n",
    "        #                                                                         #\n",
    "        # You should replace 10000 with max_len here.\n",
    "        ###########################################################################\n",
    "        #Finding even values\n",
    "        even = torch.arange(0, self.input_dim, 2)\n",
    "        #Since odd den = even den\n",
    "        den = self.max_len ** (even/self.input_dim)\n",
    "        #Pos as ascending values from 0 to seq_len\n",
    "        pos = torch.arange(seq_len).reshape(seq_len, 1)\n",
    "        even_pe = torch.sin(pos / den)\n",
    "        odd_pe = torch.cos(pos / den)\n",
    "        #Stacking odd and even together\n",
    "        even_odd_stack = torch.stack([even_pe, odd_pe], dim=2)\n",
    "        pe = torch.flatten(even_odd_stack, 1, 2)\n",
    "        x = x + pe\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.044782Z",
     "iopub.status.busy": "2023-04-25T03:06:57.044241Z",
     "iopub.status.idle": "2023-04-25T03:06:57.066329Z",
     "shell.execute_reply": "2023-04-25T03:06:57.064656Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.044719Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
    "          the same dimensions. But they could have different dimensions in other problems.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert input_dim % num_heads == 0\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = input_dim // num_heads\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Define the linear transformation layers for key, value, and query.#\n",
    "        # Also define the output layer.\n",
    "        ###########################################################################\n",
    "        self.key = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.value = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.query = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.output = nn.Linear(self.input_dim,self.input_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "        \n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - key: Tensor of the shape BxLxC\n",
    "        - value: Tensor of the shape BxLxC\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]        \n",
    "        \n",
    "        dot_prod_scores = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the scores based on dot product between transformed query,#\n",
    "        # key, and value. You may find torch.matmul helpful, whose documentation  #\n",
    "        # can be found at                                                         #\n",
    "        # https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul#\n",
    "        # Remember to devide the doct product similarity scores by square root of #\n",
    "        # the channel dimension per head.   \n",
    "        #                                                                         #\n",
    "        # Since no for loops are allowed here, think of how to use tensor reshape #\n",
    "        # to process multiple attention heads at the same time.                   #\n",
    "        ###########################################################################\n",
    "        \n",
    "        q = self.query(query)\n",
    "        q = torch.reshape(q,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        q = q.transpose(1,2)\n",
    "        #print('qshape',q.shape)\n",
    "        k = self.key(key)\n",
    "        k = torch.reshape(k,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        k = k.transpose(1,2)\n",
    "        #print('kshape',k.shape)\n",
    "        v = self.value(value)\n",
    "        v = torch.reshape(v,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        v = v.transpose(1,2)\n",
    "        #print('vshape',v.shape)\n",
    "        \n",
    "        key_t = k.transpose(3,2)\n",
    "        #print(key_t.shape)\n",
    "        dot_prod_scores = torch.matmul(q,key_t)\n",
    "        dot_prod_scores = dot_prod_scores / math.sqrt(self.dim_per_head)\n",
    "        #print(dot_prod_scores.shape)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        if mask is not None:\n",
    "            # We simply set the similarity scores to be near zero for the positions\n",
    "            # where the attention should not be done. Think of why we do this.\n",
    "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        out = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the attention scores, which are then used to modulate the #\n",
    "        # value tensor. Finally concate the attended tensors from multiple heads  #\n",
    "        # and feed it into the output layer. You may still find torch.matmul      #\n",
    "        # helpful.                                                                #\n",
    "        #                                                                         #\n",
    "        # Again, think of how to use reshaping tensor to do the concatenation.    #\n",
    "        ###########################################################################\n",
    "        S = torch.nn.Softmax(dim = -1)\n",
    "        scores = S(dot_prod_scores)\n",
    "        #print(scores.shape)\n",
    "        scores = torch.matmul(scores, v)\n",
    "        #print(scores.shape)\n",
    "        out = scores.transpose(1,2)\n",
    "        out = torch.reshape(out,(b,-1,self.num_heads*self.dim_per_head))\n",
    "        out = self.output(out)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.069599Z",
     "iopub.status.busy": "2023-04-25T03:06:57.069034Z",
     "iopub.status.idle": "2023-04-25T03:06:57.092754Z",
     "shell.execute_reply": "2023-04-25T03:06:57.090963Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.069551Z"
    }
   },
   "outputs": [],
   "source": [
    "class Text_Guided_Self_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
    "          the same dimensions. But they could have different dimensions in other problems.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(Text_Guided_Self_Attention, self).__init__()\n",
    "        \n",
    "        assert input_dim % num_heads == 0\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = input_dim // num_heads\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Define the linear transformation layers for key, value, and query.#\n",
    "        # Also define the output layer.\n",
    "        ###########################################################################\n",
    "        self.key = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.value = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.query = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.y_text = nn.Linear(self.input_dim,self.input_dim)\n",
    "        self.output = nn.Linear(self.input_dim,self.input_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, y_text: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "        \n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - key: Tensor of the shape BxLxC\n",
    "        - value: Tensor of the shape BxLxC\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]        \n",
    "        \n",
    "        dot_prod_scores = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the scores based on dot product between transformed query,#\n",
    "        # key, and value. You may find torch.matmul helpful, whose documentation  #\n",
    "        # can be found at                                                         #\n",
    "        # https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul#\n",
    "        # Remember to devide the doct product similarity scores by square root of #\n",
    "        # the channel dimension per head.   \n",
    "        #                                                                         #\n",
    "        # Since no for loops are allowed here, think of how to use tensor reshape #\n",
    "        # to process multiple attention heads at the same time.                   #\n",
    "        ###########################################################################\n",
    "        \n",
    "        q = self.query(query)\n",
    "        q = torch.reshape(q,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        q = q.transpose(1,2)\n",
    "        \n",
    "        #q_cap\n",
    "        y_t = self.y_text(y_text)\n",
    "        y_t = torch.reshape(y_t,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        y_t = y_t.transpose(1,2)\n",
    "        y_t_t = y_t.transpose(3,2)\n",
    "        q_temp = torch.matmul(q,y_t_t)\n",
    "        q_temp = q_temp / math.sqrt(self.dim_per_head)\n",
    "        S = torch.nn.Softmax(dim = -1)\n",
    "        q_cap = S(q_temp)\n",
    "        q_cap = torch.matmul(q_cap, y_t)\n",
    "        \n",
    "        #print('qshape',q.shape)\n",
    "        k = self.key(key)\n",
    "        k = torch.reshape(k,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        k = k.transpose(1,2)\n",
    "        #print('kshape',k.shape)\n",
    "        v = self.value(value)\n",
    "        v = torch.reshape(v,(b, -1,self.num_heads,self.dim_per_head)) \n",
    "        v = v.transpose(1,2)\n",
    "        #print('vshape',v.shape)\n",
    "        \n",
    "        key_t = k.transpose(3,2)\n",
    "        #print(key_t.shape)\n",
    "        dot_prod_scores = torch.matmul(q_cap,key_t)\n",
    "        dot_prod_scores = dot_prod_scores / math.sqrt(self.dim_per_head)\n",
    "        #print(dot_prod_scores.shape)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        if mask is not None:\n",
    "            # We simply set the similarity scores to be near zero for the positions\n",
    "            # where the attention should not be done. Think of why we do this.\n",
    "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        out = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the attention scores, which are then used to modulate the #\n",
    "        # value tensor. Finally concate the attended tensors from multiple heads  #\n",
    "        # and feed it into the output layer. You may still find torch.matmul      #\n",
    "        # helpful.                                                                #\n",
    "        #                                                                         #\n",
    "        # Again, think of how to use reshaping tensor to do the concatenation.    #\n",
    "        ###########################################################################\n",
    "        S = torch.nn.Softmax(dim = -1)\n",
    "        scores = S(dot_prod_scores)\n",
    "        #print(scores.shape)\n",
    "        scores = torch.matmul(scores, v)\n",
    "        #print(scores.shape)\n",
    "        out = scores.transpose(1,2)\n",
    "        out = torch.reshape(out,(b,-1,self.num_heads*self.dim_per_head))\n",
    "        out = self.output(out)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.095064Z",
     "iopub.status.busy": "2023-04-25T03:06:57.094572Z",
     "iopub.status.idle": "2023-04-25T03:06:57.113692Z",
     "shell.execute_reply": "2023-04-25T03:06:57.112633Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.095030Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, ff_dim, dropout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension\n",
    "        - ff_dim: Hidden dimension\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Define the two linear layers and a non-linear one.\n",
    "        ###########################################################################\n",
    "        self.ll1 = nn.Linear(input_dim, ff_dim)\n",
    "        self.nl = nn.ReLU()\n",
    "        self.ll2 = nn.Linear(ff_dim, input_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "         and C is the channel dimension\n",
    "          \n",
    "        Return:\n",
    "        - y: Tensor of the shape BxLxC\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Process the input.                                                #\n",
    "        ###########################################################################\n",
    "        l1_out = self.ll1(x)\n",
    "        n_out = self.nl(l1_out)\n",
    "        y = self.ll2(n_out)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.115725Z",
     "iopub.status.busy": "2023-04-25T03:06:57.115378Z",
     "iopub.status.idle": "2023-04-25T03:06:57.135168Z",
     "shell.execute_reply": "2023-04-25T03:06:57.133752Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.115690Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_emb_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TextEncoderCell, self).__init__()\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: A single Transformer encoder cell consists of \n",
    "        # 1. A multi-head attention module\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
    "        #                                                                         #\n",
    "        # At the same time, it also has\n",
    "        # 1. A feedforward network\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm\n",
    "        ###########################################################################\n",
    "        self.textual_self_attention = MultiHeadAttention(word_emb_dim,num_heads)\n",
    "        self.dropout1_text = torch.nn.Dropout(dropout)\n",
    "        self.norm1_text = torch.nn.LayerNorm(word_emb_dim)\n",
    "        self.feed_forward_text = FeedForwardNetwork(word_emb_dim,ff_dim,dropout)\n",
    "        self.dropout2_text = torch.nn.Dropout(dropout)\n",
    "        self.norm2_text = torch.nn.LayerNorm(word_emb_dim)\n",
    "        \n",
    "\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def forward(self,text:torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "        \n",
    "        y_text = None\n",
    "        ###########################################################################\n",
    "        # TODO: Get the output of the multi-head attention part (with dropout     #\n",
    "        # and layer norm), which is used as input to the feedforward network (    #\n",
    "        # again, followed by dropout and layer norm).                             #\n",
    "        #                                                                         #\n",
    "        # Don't forget the residual connections for both parts.                   #\n",
    "        ###########################################################################\n",
    "\n",
    "        attention_op_text = self.textual_self_attention(text,text,text,mask)\n",
    "        dropout1_op_text = self.dropout1_text(attention_op_text)\n",
    "        norm1_op_text = self.norm1_text(text + dropout1_op_text)\n",
    "\n",
    "        feed_forward_op_text = self.feed_forward_text(norm1_op_text)\n",
    "        dropout2_op_text = self.dropout2_text(feed_forward_op_text)\n",
    "        norm2_op_text = self.norm2_text(norm1_op_text + dropout2_op_text)\n",
    "        y_text = norm2_op_text\n",
    "        \n",
    "        \n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return y_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.137416Z",
     "iopub.status.busy": "2023-04-25T03:06:57.136571Z",
     "iopub.status.idle": "2023-04-25T03:06:57.156761Z",
     "shell.execute_reply": "2023-04-25T03:06:57.155317Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.137370Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImgEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_emb_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(ImgEncoderCell, self).__init__()\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: A single Transformer encoder cell consists of \n",
    "        # 1. A multi-head attention module\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
    "        #                                                                         #\n",
    "        # At the same time, it also has\n",
    "        # 1. A feedforward network\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm\n",
    "        ###########################################################################\n",
    "        self.text_guided_self_attention = Text_Guided_Self_Attention(img_emb_dim,num_heads)\n",
    "        self.dropout1_img = torch.nn.Dropout(dropout)\n",
    "        self.norm1_img = torch.nn.LayerNorm(img_emb_dim)\n",
    "        self.feed_forward_img = FeedForwardNetwork(img_emb_dim,ff_dim,dropout)\n",
    "        self.dropout2_img = torch.nn.Dropout(dropout)\n",
    "        self.norm2_img = torch.nn.LayerNorm(img_emb_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def forward(self, img: torch.Tensor, y_text:torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        y_img = None\n",
    "        ###########################################################################\n",
    "        # TODO: Get the output of the multi-head attention part (with dropout     #\n",
    "        # and layer norm), which is used as input to the feedforward network (    #\n",
    "        # again, followed by dropout and layer norm).                             #\n",
    "        #                                                                         #\n",
    "        # Don't forget the residual connections for both parts.                   #\n",
    "        ###########################################################################\n",
    "        \n",
    "        attention_op_img = self.text_guided_self_attention(img,img,img,y_text,mask)\n",
    "        dropout1_op_img = self.dropout1_img(attention_op_img)\n",
    "        norm1_op_img = self.norm1_img(img + dropout1_op_img)\n",
    "\n",
    "        feed_forward_op_img = self.feed_forward_img(norm1_op_img)\n",
    "        dropout2_op_img = self.dropout2_img(feed_forward_op_img)\n",
    "        norm2_op_img = self.norm2_img(norm1_op_img + dropout2_op_img)\n",
    "        y_img = norm2_op_img\n",
    "        \n",
    "        \n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.159553Z",
     "iopub.status.busy": "2023-04-25T03:06:57.158557Z",
     "iopub.status.idle": "2023-04-25T03:06:57.176541Z",
     "shell.execute_reply": "2023-04-25T03:06:57.175303Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.159501Z"
    }
   },
   "outputs": [],
   "source": [
    "class GroundingEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_emb_dim: int, word_emb_dim: int, num_heads: int, ff_dim: int=768, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(GroundingEncoder, self).__init__()\n",
    "        \n",
    "        self.norm = None\n",
    "        ###########################################################################\n",
    "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
    "        # TranformerEncoderCells. Check the documentation here of how to use it   #\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
    "        \n",
    "        # At the same time, define a layer normalization layer to process the     #\n",
    "        # output of the entire encoder.                                           #\n",
    "        ###########################################################################\n",
    "        self.text_cell = TextEncoderCell(word_emb_dim, num_heads, ff_dim, dropout)\n",
    "        self.img_cell = ImgEncoderCell(img_emb_dim,num_heads, ff_dim, dropout) \n",
    "        self.norm = torch.nn.LayerNorm(img_emb_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def forward(self, x_img_pe: torch.Tensor,x_text: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Feed x into the stack of TransformerEncoderCells and then         #\n",
    "        # normalize the output with layer norm.                                   #\n",
    "        ###########################################################################\n",
    "        x_text = self.text_cell(x_text,mask)\n",
    "        x_img = self.img_cell(x_img_pe,x_text,mask)\n",
    "        y_text = self.norm(x_text)\n",
    "        y_img = self.norm(x_img)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return y_text,y_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.178905Z",
     "iopub.status.busy": "2023-04-25T03:06:57.178227Z",
     "iopub.status.idle": "2023-04-25T03:06:57.196830Z",
     "shell.execute_reply": "2023-04-25T03:06:57.195470Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.178864Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based text classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            img_emb_dim:int, word_emb_dim: int, num_heads: int, trx_ff_dim: int, \n",
    "         dropout: float=0.1, pad_token: int=0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
    "        - embed_dim: The dimension of word embeddings\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - trx_ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_trx_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio\n",
    "        - pad_token: The index of the padding token.\n",
    "        \"\"\"\n",
    "        super(VGEncoder, self).__init__()\n",
    "        \n",
    "        self.img_emb_dim = img_emb_dim\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Define a module for positional encoding, Transformer encoder, and #\n",
    "        # a output layer                                                          #\n",
    "        ###########################################################################\n",
    "        self.positional_encoding = PositionalEncoding(img_emb_dim)\n",
    "        self.grounding_encoder = GroundingEncoder(word_emb_dim, img_emb_dim, num_heads, trx_ff_dim, dropout )\n",
    "        self.output_layer = torch.nn.Linear(img_emb_dim,768)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, img_emb, text_emb, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - text: Tensor with the shape of BxLxC.\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \n",
    "        Return:\n",
    "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Apply positional embedding to the input, which is then fed into   #\n",
    "        # the encoder. Average pooling is applied then to all the features of all #\n",
    "        # tokens. Finally, the logits are computed based on the pooled features.  #\n",
    "        ###########################################################################\n",
    "        positional_encoded_img = self.positional_encoding(img_emb)\n",
    "        grounding_encoder_text, grounding_encoder_img = self.grounding_encoder(positional_encoded_img,text_emb)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return grounding_encoder_text, grounding_encoder_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.198908Z",
     "iopub.status.busy": "2023-04-25T03:06:57.198515Z",
     "iopub.status.idle": "2023-04-25T03:06:57.217703Z",
     "shell.execute_reply": "2023-04-25T03:06:57.216650Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.198870Z"
    }
   },
   "outputs": [],
   "source": [
    "class GroundingDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) of the Transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int=768, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(GroundingDecoderCell, self).__init__()\n",
    "        \n",
    "        ###########################################################################\n",
    "        # TODO: Similar to the TransformerEncoderCell, define two                 #\n",
    "        # MultiHeadAttention modules. One for processing the tokens on the        # \n",
    "        # decoder side. The other for getting the attention across the encoder.   #\n",
    "        # and the decoder. Also define a feedforward network. Don't forget the    #\n",
    "        # Dropout and Layer Norm layers.                                          #\n",
    "        ###########################################################################\n",
    "        self.grounding_query_self_attention = MultiHeadAttention(input_dim,num_heads)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(input_dim)\n",
    "        self.encoder_decoder_self_attention = MultiHeadAttention(input_dim,num_heads)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "        self.norm2 = torch.nn.LayerNorm(input_dim)\n",
    "        self.feed_forward = FeedForwardNetwork(input_dim,ff_dim,dropout)\n",
    "        self.dropout3 = torch.nn.Dropout(dropout)\n",
    "        self.norm3 = torch.nn.LayerNorm(input_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def forward(self, grounding_query: torch.Tensor, encoder_img: torch.Tensor, src_mask=None, tgt_mask=None):            \n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the self-attended features for the tokens on the decoder  #\n",
    "        # side. Then compute the corss-attended features for the tokens on the    #\n",
    "        # decoder side to the encoded features, which are finally feed into the   #\n",
    "        # feedforward network                                                     #\n",
    "        ###########################################################################\n",
    "        attention_op1 = self.grounding_query_self_attention(grounding_query,grounding_query,grounding_query,tgt_mask)\n",
    "        attention_residual_op1 = attention_op1 + grounding_query\n",
    "        dropout1_op = self.dropout1(attention_residual_op1)\n",
    "        norm1_op = self.norm1(dropout1_op)\n",
    "        \n",
    "        attention_op2 = self.encoder_decoder_self_attention(grounding_query,encoder_img,encoder_img,src_mask)\n",
    "        attention_residual_op2 = attention_op2 + norm1_op\n",
    "        dropout2_op = self.dropout2(attention_residual_op2)\n",
    "        norm2_op = self.norm2(dropout2_op)\n",
    "        \n",
    "        feed_forward_op = self.feed_forward(norm2_op)\n",
    "        feed_forward_residual_op = feed_forward_op + norm2_op\n",
    "        dropout3_op = self.dropout3(feed_forward_residual_op)\n",
    "        norm3_op = self.norm3(dropout3_op)\n",
    "        y = norm3_op\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.220581Z",
     "iopub.status.busy": "2023-04-25T03:06:57.219414Z",
     "iopub.status.idle": "2023-04-25T03:06:57.237431Z",
     "shell.execute_reply": "2023-04-25T03:06:57.236129Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.220527Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGDecoder(nn.Module):\n",
    "   \n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout=0.1):\n",
    "        \n",
    "        super(VGDecoder, self).__init__()\n",
    "        self.d_cell = GroundingDecoderCell(input_dim, num_heads, ff_dim, dropout)\n",
    "        self.norm_l = torch.nn.LayerNorm(input_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, grounding_query: torch.Tensor, encoder_img: torch.Tensor, src_mask=None, tgt_mask=None):            \n",
    "        y = None\n",
    "        cell_output = self.d_cell(grounding_query,encoder_img,src_mask,tgt_mask)\n",
    "        y = self.norm_l(cell_output)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.244573Z",
     "iopub.status.busy": "2023-04-25T03:06:57.244154Z",
     "iopub.status.idle": "2023-04-25T03:06:57.256335Z",
     "shell.execute_reply": "2023-04-25T03:06:57.255039Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.244533Z"
    }
   },
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "\n",
    "    def __init__(self,img_emb_dim:int, word_emb_dim: int, num_heads: int, trx_ff_dim: int\n",
    "                 ,hidden_dim, dropout: float=0.1, pad_token: int=0):\n",
    "        super().__init__()\n",
    "        self.visual_grounding_encoder = VGEncoder(img_emb_dim, word_emb_dim, num_heads, trx_ff_dim,dropout=0.1)\n",
    "        self.visual_grounding_decoder = VGDecoder(word_emb_dim, num_heads, trx_ff_dim, dropout=0.1)\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(word_emb_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,4)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_emb, word_emb):\n",
    "\n",
    "        grounding_encoder_text, grounding_encoder_img = self.visual_grounding_encoder(img_emb, word_emb)\n",
    "        #print(grounding_encoder_img.size())\n",
    "        transformed_emb = self.visual_grounding_decoder(grounding_encoder_text, grounding_encoder_img)\n",
    "        #print(transformed_emb.size())\n",
    "        pred = self.prediction_head(transformed_emb).sigmoid()\n",
    "        #print(pred.size())\n",
    "        pred = pred.squeeze(1)\n",
    "        return pred\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.258479Z",
     "iopub.status.busy": "2023-04-25T03:06:57.257982Z",
     "iopub.status.idle": "2023-04-25T03:06:57.276961Z",
     "shell.execute_reply": "2023-04-25T03:06:57.275832Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.258424Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val*n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def xywh2xyxy(x):  # Convert bounding box format from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "    y = torch.zeros(x.shape) if x.dtype is torch.float32 else np.zeros(x.shape)\n",
    "    y[:, 0] = (x[:, 0] - x[:, 2] / 2)\n",
    "    y[:, 1] = (x[:, 1] - x[:, 3] / 2)\n",
    "    y[:, 2] = (x[:, 0] + x[:, 2] / 2)\n",
    "    y[:, 3] = (x[:, 1] + x[:, 3] / 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.279098Z",
     "iopub.status.busy": "2023-04-25T03:06:57.278406Z",
     "iopub.status.idle": "2023-04-25T03:06:57.296848Z",
     "shell.execute_reply": "2023-04-25T03:06:57.295462Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.279057Z"
    }
   },
   "outputs": [],
   "source": [
    "class Criterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.loss_weight = [3, 1]\n",
    "        self.MSELoss = torch.nn.MSELoss(reduction='none')\n",
    "    def forward(self, pred, gt, img_size=256):\n",
    "        \"\"\"\n",
    "        :param pred:  (bs, 4)\n",
    "        :param gt: (bs, 4)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        bs = pred.shape[0]\n",
    "        gt = gt / img_size\n",
    "\n",
    "        loss_bbox = F.l1_loss(pred, gt, reduction='none')\n",
    "        loss_bbox = loss_bbox.sum() / bs\n",
    "\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou_loss(\n",
    "                                   self.box_cxcywh_to_xyxy(pred),\n",
    "                                   self.box_cxcywh_to_xyxy(gt)))\n",
    "\n",
    "        loss_giou = loss_giou.sum() / bs\n",
    "        loss = 5 * loss_bbox + loss_giou * 2\n",
    "        return loss, 5 * loss_bbox, loss_giou * 2\n",
    "    \n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "        x_c, y_c, w, h = x.unbind(-1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.299339Z",
     "iopub.status.busy": "2023-04-25T03:06:57.298659Z",
     "iopub.status.idle": "2023-04-25T03:06:57.335820Z",
     "shell.execute_reply": "2023-04-25T03:06:57.334455Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.299298Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, optimizer, epoch, criterion=None, img_size=512):\n",
    "    bs =32\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    losses_bbox = AverageMeter()\n",
    "    losses_giou = AverageMeter()\n",
    "\n",
    "    acc = AverageMeter()\n",
    "    miou = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        imgs = batch[0]\n",
    "        word_id = batch[1]\n",
    "        img_emb = batch[2]\n",
    "        word_emb =batch[3]\n",
    "        bbox = batch[4]\n",
    "        #bbox = torch.clamp(bbox, min=0, max=(512 - 1))\n",
    "        image_emb = Variable(img_emb.unsqueeze(1))\n",
    "        word_emb = Variable(word_emb.unsqueeze(1))\n",
    "        bbox = Variable(bbox)\n",
    "\n",
    "        norm_bbox = torch.zeros_like(bbox)\n",
    "\n",
    "        norm_bbox[:, 0] = (bbox[:, 0] + bbox[:, 2]) / 2.0  # x_center\n",
    "        norm_bbox[:, 1] = (bbox[:, 1] + bbox[:, 3]) / 2.0  # y_center\n",
    "        norm_bbox[:, 2] = bbox[:, 2] - bbox[:, 0]   # w\n",
    "        norm_bbox[:, 3] = bbox[:, 3] - bbox[:, 1]    # h\n",
    "        #print(norm_bbox)\n",
    "\n",
    "        # forward\n",
    "        pred_box = model(image_emb, word_emb)\n",
    "        #print(pred_box.size())# [bs, C, H, W]\n",
    "        loss, loss_box, loss_giou = criterion(pred_box, norm_bbox, img_size=img_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # pred-box\n",
    "        pred_bbox = pred_box.detach()\n",
    "        pred_bbox = pred_bbox * img_size\n",
    "        pred_box = xywh2xyxy(pred_bbox)\n",
    "\n",
    "        losses.update(loss.item(), bs)\n",
    "        losses_bbox.update(loss_box.item(), bs)\n",
    "        losses_giou.update(loss_giou.item(), bs)\n",
    "\n",
    "        target_bbox = bbox\n",
    "        iou = box_iou(pred_box, target_bbox.data)\n",
    "#         print(\"in here\")\n",
    "        \n",
    "        accu = np.sum(np.array((iou.data.numpy() > 0.5), dtype=float)) / bs\n",
    "\n",
    "        # metrics\n",
    "        miou.update(torch.mean(iou).item(), image_emb.size(0))\n",
    "        acc.update(accu, image_emb.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (batch_idx%300)== 0 :\n",
    "            print_str = 'Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                        'Loss_bbox {loss_box.val:.4f} ({loss_box.avg:.4f})\\t' \\\n",
    "                        'Loss_giou {loss_giou.val:.4f} ({loss_giou.avg:.4f})\\t' \\\n",
    "                        'Accu {acc.val:.4f} ({acc.avg:.4f})\\t' \\\n",
    "                        'Mean_iu {miou.val:.4f} ({miou.avg:.4f})\\t' \\\n",
    "                .format(epoch+1, batch_idx+1, len(train_loader),\n",
    "                        batch_time=batch_time,\n",
    "                        loss=losses,\n",
    "                        loss_box=losses_bbox,\n",
    "                        loss_giou=losses_giou,\n",
    "                        acc=acc,\n",
    "                        miou=miou)\n",
    "\n",
    "            print(print_str)\n",
    "            \n",
    "\n",
    "def validate_epoch(val_loader, model, train_epoch, img_size=512):\n",
    "    bs=32\n",
    "    batch_time = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    miou = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx,batch in enumerate(val_loader):\n",
    "        imgs = batch[0]\n",
    "        word_id = batch[1]\n",
    "        img_emb = batch[2]\n",
    "        word_emb =batch[3]\n",
    "        bbox = batch[4]\n",
    "        #bbox = torch.clamp(bbox, min=0, max=(512 - 1))\n",
    "        \n",
    "        image_emb = Variable(img_emb.unsqueeze(1))\n",
    "        word_emb = Variable(word_emb.unsqueeze(1))\n",
    "        bbox = Variable(bbox)\n",
    "\n",
    "        norm_bbox = torch.zeros_like(bbox)\n",
    "\n",
    "        norm_bbox[:, 0] = (bbox[:, 0] + bbox[:, 2]) / 2.0  # x_center\n",
    "        norm_bbox[:, 1] = (bbox[:, 1] + bbox[:, 3]) / 2.0  # y_center\n",
    "        norm_bbox[:, 2] = bbox[:, 2] - bbox[:, 0]   # w\n",
    "        norm_bbox[:, 3] = bbox[:, 3] - bbox[:, 1]    # h\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_box = model(image_emb, word_emb)  # [bs, C, H, W]\n",
    "            \n",
    "\n",
    "        pred_bbox = pred_box.detach()\n",
    "        pred_bbox = pred_bbox * img_size\n",
    "        pred_bbox = xywh2xyxy(pred_bbox)\n",
    "\n",
    "        # constrain\n",
    "        pred_bbox[pred_bbox < 0.0] = 0.0\n",
    "        pred_bbox[pred_bbox > img_size-1] = img_size-1\n",
    "\n",
    "        target_bbox = bbox\n",
    "        # metrics\n",
    "        iou = box_iou(pred_bbox, target_bbox.data)\n",
    "        # accu = np.sum(np.array((iou.data.cpu().numpy() > 0.5), dtype=float)) / args.batch_size\n",
    "        accu = np.sum(np.array((iou.data.cpu().numpy() > 0.5), dtype=float)) / bs\n",
    "\n",
    "        acc.update(accu, bs)\n",
    "        miou.update(torch.mean(iou).item(), bs)\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (batch_idx%100) == 0:\n",
    "            print_str = 'Validate: [{0}/{1}]\\t' \\\n",
    "                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  ' \\\n",
    "                        'Acc {acc.val:.4f} ({acc.avg:.4f})  ' \\\n",
    "                        'Mean_iu {miou.val:.4f} ({miou.avg:.4f})  ' \\\n",
    "                .format(batch_idx+1, len(val_loader), batch_time=batch_time, acc=acc, miou=miou)\n",
    "\n",
    "            print(print_str)\n",
    "            \n",
    "    print(f\"Train_epoch {train_epoch+1}  Validate Result:  Acc {acc.avg}, MIoU {miou.avg}.\")\n",
    "\n",
    "\n",
    "    return acc.avg, miou.avg\n",
    "\n",
    "def test_epoch(test_loader, model, img_size=512):\n",
    "    bs = 32\n",
    "    acc = AverageMeter()\n",
    "    miou = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        imgs = batch[0]\n",
    "        word_id = batch[1]\n",
    "        img_emb = batch[2]\n",
    "        word_emb =batch[3]\n",
    "        bbox = batch[4]\n",
    "        #bbox = torch.clamp(bbox, min=0, max=(512 - 1))\n",
    "        image_emb = Variable(img_emb.unsqueeze(1))\n",
    "        word_emb = Variable(word_emb.unsqueeze(1))\n",
    "        bbox = Variable(bbox)\n",
    "        norm_bbox = torch.zeros_like(bbox)\n",
    "\n",
    "        norm_bbox[:, 0] = (bbox[:, 0] + bbox[:, 2]) / 2.0  # x_center\n",
    "        norm_bbox[:, 1] = (bbox[:, 1] + bbox[:, 3]) / 2.0  # y_center\n",
    "        norm_bbox[:, 2] = bbox[:, 2] - bbox[:, 0]   # w\n",
    "        norm_bbox[:, 3] = bbox[:, 3] - bbox[:, 1]    # h\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_box = model(image_emb, word_emb)  # [bs, C, H, W]\n",
    "\n",
    "        pred_bbox = pred_box.detach()\n",
    "        pred_bbox = pred_bbox * img_size\n",
    "        pred_bbox = xywh2xyxy(pred_bbox)\n",
    "\n",
    "        # constrain\n",
    "        pred_bbox[pred_bbox < 0.0] = 0.0\n",
    "        pred_bbox[pred_bbox > img_size-1] = img_size-1\n",
    "\n",
    "        target_bbox = bbox\n",
    "        # metrics\n",
    "        iou = box_iou(pred_bbox, target_bbox.data)\n",
    "        accu = np.sum(np.array((iou.data.cpu().numpy() > 0.5), dtype=float)) / bs\n",
    "\n",
    "        acc.update(accu, bs)\n",
    "        miou.update(torch.mean(iou).item(), bs)\n",
    "    print(f\"Test Result:  Acc {acc.avg}, MIoU {miou.avg}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T03:06:57.338583Z",
     "iopub.status.busy": "2023-04-25T03:06:57.337421Z",
     "iopub.status.idle": "2023-04-25T14:35:25.387753Z",
     "shell.execute_reply": "2023-04-25T14:35:25.386204Z",
     "shell.execute_reply.started": "2023-04-25T03:06:57.338537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/1843]\tTime 0.565 (0.565)\tLoss 66.9724 (66.9724)\tLoss_bbox 4.9187 (4.9187)\tLoss_giou 62.0537 (62.0537)\tAccu 0.4688 (0.4688)\tMean_iu 0.1325 (0.1325)\t\n",
      "Epoch: [1][301/1843]\tTime 0.244 (0.250)\tLoss 65.2144 (65.1017)\tLoss_bbox 3.5418 (3.2841)\tLoss_giou 61.6727 (61.8176)\tAccu 0.2500 (0.5257)\tMean_iu 0.0766 (0.1173)\t\n",
      "Epoch: [1][601/1843]\tTime 0.255 (0.252)\tLoss 64.9198 (65.0466)\tLoss_bbox 2.9135 (3.2318)\tLoss_giou 62.0063 (61.8149)\tAccu 1.7812 (0.5519)\tMean_iu 0.1676 (0.1159)\t\n",
      "Epoch: [1][901/1843]\tTime 0.268 (0.261)\tLoss 64.9652 (64.9955)\tLoss_bbox 3.2436 (3.1818)\tLoss_giou 61.7217 (61.8137)\tAccu 0.3125 (0.5413)\tMean_iu 0.0994 (0.1147)\t\n",
      "Epoch: [1][1201/1843]\tTime 0.374 (0.266)\tLoss 64.5128 (64.9548)\tLoss_bbox 2.7934 (3.1355)\tLoss_giou 61.7194 (61.8193)\tAccu 0.0312 (0.5574)\tMean_iu 0.1032 (0.1154)\t\n",
      "Epoch: [1][1501/1843]\tTime 1.504 (0.392)\tLoss 64.7178 (64.9240)\tLoss_bbox 3.0097 (3.1004)\tLoss_giou 61.7082 (61.8235)\tAccu 0.0625 (0.5734)\tMean_iu 0.0961 (0.1163)\t\n",
      "Epoch: [1][1801/1843]\tTime 3.551 (0.914)\tLoss 64.6589 (64.8990)\tLoss_bbox 2.8716 (3.0742)\tLoss_giou 61.7873 (61.8248)\tAccu 0.9062 (0.5760)\tMean_iu 0.1122 (0.1163)\t\n",
      "Validate: [1/370]\tTime 3.252 (3.252)  Acc 0.0625 (0.0625)  Mean_iu 0.0765 (0.0765)  \n",
      "Validate: [101/370]\tTime 3.225 (3.244)  Acc 0.5625 (0.4988)  Mean_iu 0.1026 (0.1021)  \n",
      "Validate: [201/370]\tTime 3.281 (3.243)  Acc 0.4375 (0.4729)  Mean_iu 0.0863 (0.1032)  \n",
      "Validate: [301/370]\tTime 3.238 (3.244)  Acc 0.2188 (0.4612)  Mean_iu 0.0964 (0.1031)  \n",
      "Train_epoch 1  Validate Result:  Acc 0.4566722972972973, MIoU 0.10282571693328586.\n",
      "Epoch: [2][1/1843]\tTime 3.602 (3.602)\tLoss 64.4239 (64.4239)\tLoss_bbox 2.6776 (2.6776)\tLoss_giou 61.7464 (61.7464)\tAccu 0.7188 (0.7188)\tMean_iu 0.1054 (0.1054)\t\n",
      "Epoch: [2][301/1843]\tTime 3.074 (3.287)\tLoss 64.5388 (64.7396)\tLoss_bbox 2.5451 (2.9020)\tLoss_giou 61.9937 (61.8377)\tAccu 1.1562 (0.6645)\tMean_iu 0.1347 (0.1177)\t\n",
      "Epoch: [2][601/1843]\tTime 2.892 (3.150)\tLoss 64.2886 (64.7383)\tLoss_bbox 2.5335 (2.9035)\tLoss_giou 61.7551 (61.8348)\tAccu 0.8750 (0.6447)\tMean_iu 0.1043 (0.1173)\t\n",
      "Epoch: [2][901/1843]\tTime 2.996 (3.059)\tLoss 64.6546 (64.7302)\tLoss_bbox 2.7669 (2.9016)\tLoss_giou 61.8877 (61.8286)\tAccu 0.9688 (0.6319)\tMean_iu 0.1265 (0.1166)\t\n",
      "Epoch: [2][1201/1843]\tTime 2.818 (2.999)\tLoss 64.7705 (64.7253)\tLoss_bbox 2.8995 (2.8940)\tLoss_giou 61.8710 (61.8313)\tAccu 0.8438 (0.6423)\tMean_iu 0.1242 (0.1173)\t\n",
      "Epoch: [2][1501/1843]\tTime 2.804 (2.963)\tLoss 64.5502 (64.7198)\tLoss_bbox 2.9562 (2.8867)\tLoss_giou 61.5940 (61.8332)\tAccu 0.1250 (0.6490)\tMean_iu 0.0677 (0.1175)\t\n",
      "Epoch: [2][1801/1843]\tTime 2.828 (2.940)\tLoss 64.0910 (64.7176)\tLoss_bbox 2.5592 (2.8801)\tLoss_giou 61.5318 (61.8375)\tAccu 0.3438 (0.6598)\tMean_iu 0.0697 (0.1183)\t\n",
      "Validate: [1/370]\tTime 2.553 (2.553)  Acc 1.0000 (1.0000)  Mean_iu 0.1066 (0.1066)  \n",
      "Validate: [101/370]\tTime 2.550 (2.560)  Acc 1.7188 (0.9734)  Mean_iu 0.1678 (0.1386)  \n",
      "Validate: [201/370]\tTime 2.535 (2.559)  Acc 0.8750 (0.9333)  Mean_iu 0.1307 (0.1367)  \n",
      "Validate: [301/370]\tTime 2.541 (2.557)  Acc 0.8750 (0.9427)  Mean_iu 0.1399 (0.1367)  \n",
      "Train_epoch 2  Validate Result:  Acc 0.9203547297297298, MIoU 0.135932371503598.\n",
      "Epoch: [3][1/1843]\tTime 2.870 (2.870)\tLoss 64.5394 (64.5394)\tLoss_bbox 2.7433 (2.7433)\tLoss_giou 61.7961 (61.7961)\tAccu 0.2500 (0.2500)\tMean_iu 0.1138 (0.1138)\t\n",
      "Epoch: [3][301/1843]\tTime 2.849 (2.819)\tLoss 64.5690 (64.7208)\tLoss_bbox 2.4918 (2.8688)\tLoss_giou 62.0771 (61.8520)\tAccu 0.9062 (0.7053)\tMean_iu 0.1640 (0.1210)\t\n",
      "Epoch: [3][601/1843]\tTime 2.930 (2.826)\tLoss 64.9799 (64.6912)\tLoss_bbox 3.0200 (2.8463)\tLoss_giou 61.9600 (61.8448)\tAccu 0.3750 (0.6974)\tMean_iu 0.1276 (0.1196)\t\n",
      "Epoch: [3][901/1843]\tTime 2.864 (2.842)\tLoss 64.5254 (64.6840)\tLoss_bbox 2.8460 (2.8474)\tLoss_giou 61.6794 (61.8365)\tAccu 0.6562 (0.6865)\tMean_iu 0.1001 (0.1185)\t\n",
      "Epoch: [3][1201/1843]\tTime 2.880 (2.856)\tLoss 64.7831 (64.6831)\tLoss_bbox 2.9396 (2.8455)\tLoss_giou 61.8435 (61.8375)\tAccu 0.7812 (0.6818)\tMean_iu 0.1190 (0.1186)\t\n",
      "Epoch: [3][1501/1843]\tTime 2.900 (2.868)\tLoss 64.2746 (64.6796)\tLoss_bbox 2.3517 (2.8395)\tLoss_giou 61.9229 (61.8401)\tAccu 0.9062 (0.6859)\tMean_iu 0.1265 (0.1190)\t\n",
      "Epoch: [3][1801/1843]\tTime 2.871 (2.877)\tLoss 65.0838 (64.6791)\tLoss_bbox 3.0752 (2.8399)\tLoss_giou 62.0086 (61.8392)\tAccu 0.5938 (0.6833)\tMean_iu 0.1449 (0.1189)\t\n",
      "Validate: [1/370]\tTime 2.602 (2.602)  Acc 1.5000 (1.5000)  Mean_iu 0.1759 (0.1759)  \n",
      "Validate: [101/370]\tTime 2.587 (2.593)  Acc 1.5000 (1.1739)  Mean_iu 0.1623 (0.1565)  \n",
      "Validate: [201/370]\tTime 2.578 (2.594)  Acc 1.7500 (1.2010)  Mean_iu 0.1336 (0.1565)  \n",
      "Validate: [301/370]\tTime 2.569 (2.594)  Acc 3.6250 (1.2373)  Mean_iu 0.2522 (0.1567)  \n",
      "Train_epoch 3  Validate Result:  Acc 1.2269425675675676, MIoU 0.15599231953556472.\n",
      "Epoch: [4][1/1843]\tTime 2.966 (2.966)\tLoss 64.5533 (64.5533)\tLoss_bbox 2.7483 (2.7483)\tLoss_giou 61.8050 (61.8050)\tAccu 0.4062 (0.4062)\tMean_iu 0.0989 (0.0989)\t\n",
      "Epoch: [4][301/1843]\tTime 2.959 (2.939)\tLoss 64.8212 (64.6555)\tLoss_bbox 2.8817 (2.8219)\tLoss_giou 61.9395 (61.8336)\tAccu 1.2812 (0.6821)\tMean_iu 0.1510 (0.1199)\t\n",
      "Epoch: [4][601/1843]\tTime 2.912 (2.943)\tLoss 64.8759 (64.6519)\tLoss_bbox 2.9392 (2.8142)\tLoss_giou 61.9367 (61.8377)\tAccu 0.6562 (0.6932)\tMean_iu 0.1352 (0.1201)\t\n",
      "Epoch: [4][901/1843]\tTime 2.929 (2.939)\tLoss 64.6427 (64.6650)\tLoss_bbox 2.7562 (2.8244)\tLoss_giou 61.8865 (61.8406)\tAccu 0.9062 (0.6861)\tMean_iu 0.1395 (0.1198)\t\n",
      "Epoch: [4][1201/1843]\tTime 2.993 (2.940)\tLoss 64.7904 (64.6670)\tLoss_bbox 3.1046 (2.8263)\tLoss_giou 61.6858 (61.8406)\tAccu 0.4062 (0.6887)\tMean_iu 0.0927 (0.1197)\t\n",
      "Epoch: [4][1501/1843]\tTime 2.942 (2.940)\tLoss 64.5167 (64.6631)\tLoss_bbox 2.6888 (2.8225)\tLoss_giou 61.8279 (61.8406)\tAccu 0.5312 (0.6856)\tMean_iu 0.1196 (0.1195)\t\n",
      "Epoch: [4][1801/1843]\tTime 2.907 (2.940)\tLoss 65.0352 (64.6611)\tLoss_bbox 3.4664 (2.8223)\tLoss_giou 61.5689 (61.8388)\tAccu 0.3438 (0.6751)\tMean_iu 0.0790 (0.1191)\t\n",
      "Validate: [1/370]\tTime 2.602 (2.602)  Acc 1.3750 (1.3750)  Mean_iu 0.1337 (0.1337)  \n",
      "Validate: [101/370]\tTime 2.588 (2.607)  Acc 0.4062 (0.8252)  Mean_iu 0.1026 (0.1297)  \n",
      "Validate: [201/370]\tTime 2.600 (2.611)  Acc 1.0312 (0.8139)  Mean_iu 0.1337 (0.1286)  \n",
      "Validate: [301/370]\tTime 2.595 (2.611)  Acc 0.4062 (0.8117)  Mean_iu 0.1330 (0.1283)  \n",
      "Train_epoch 4  Validate Result:  Acc 0.8068412162162162, MIoU 0.1283639211912413.\n",
      "Epoch: [5][1/1843]\tTime 2.956 (2.956)\tLoss 64.5780 (64.5780)\tLoss_bbox 2.7040 (2.7040)\tLoss_giou 61.8740 (61.8740)\tAccu 0.7812 (0.7812)\tMean_iu 0.1214 (0.1214)\t\n",
      "Epoch: [5][301/1843]\tTime 2.876 (2.893)\tLoss 65.0044 (64.6698)\tLoss_bbox 3.0523 (2.8284)\tLoss_giou 61.9520 (61.8414)\tAccu 0.5312 (0.7109)\tMean_iu 0.1390 (0.1201)\t\n",
      "Epoch: [5][601/1843]\tTime 2.842 (2.884)\tLoss 64.6112 (64.6613)\tLoss_bbox 2.7094 (2.8236)\tLoss_giou 61.9018 (61.8376)\tAccu 1.3750 (0.7042)\tMean_iu 0.1339 (0.1192)\t\n",
      "Epoch: [5][901/1843]\tTime 2.884 (2.881)\tLoss 64.7728 (64.6651)\tLoss_bbox 3.0355 (2.8279)\tLoss_giou 61.7373 (61.8372)\tAccu 1.5312 (0.7002)\tMean_iu 0.1091 (0.1188)\t\n",
      "Epoch: [5][1201/1843]\tTime 2.930 (2.889)\tLoss 64.3486 (64.6607)\tLoss_bbox 2.4914 (2.8220)\tLoss_giou 61.8572 (61.8388)\tAccu 0.7500 (0.6879)\tMean_iu 0.1304 (0.1190)\t\n",
      "Epoch: [5][1501/1843]\tTime 2.977 (2.906)\tLoss 64.4584 (64.6602)\tLoss_bbox 2.6701 (2.8191)\tLoss_giou 61.7882 (61.8410)\tAccu 0.9062 (0.6868)\tMean_iu 0.1079 (0.1191)\t\n",
      "Epoch: [5][1801/1843]\tTime 2.932 (2.912)\tLoss 64.4022 (64.6590)\tLoss_bbox 2.6519 (2.8166)\tLoss_giou 61.7502 (61.8424)\tAccu 0.3438 (0.6852)\tMean_iu 0.0956 (0.1194)\t\n",
      "Validate: [1/370]\tTime 2.598 (2.598)  Acc 0.6875 (0.6875)  Mean_iu 0.1172 (0.1172)  \n",
      "Validate: [101/370]\tTime 2.602 (2.611)  Acc 0.6250 (0.6717)  Mean_iu 0.1303 (0.1205)  \n",
      "Validate: [201/370]\tTime 2.592 (2.615)  Acc 1.0312 (0.6864)  Mean_iu 0.1568 (0.1211)  \n",
      "Validate: [301/370]\tTime 2.613 (2.614)  Acc 1.0000 (0.6959)  Mean_iu 0.1352 (0.1209)  \n",
      "Train_epoch 5  Validate Result:  Acc 0.6963682432432432, MIoU 0.12138115322670422.\n",
      "Epoch: [6][1/1843]\tTime 2.949 (2.949)\tLoss 64.5880 (64.5880)\tLoss_bbox 2.7711 (2.7711)\tLoss_giou 61.8169 (61.8169)\tAccu 0.4688 (0.4688)\tMean_iu 0.1209 (0.1209)\t\n",
      "Epoch: [6][301/1843]\tTime 2.923 (2.906)\tLoss 64.8688 (64.6266)\tLoss_bbox 2.9933 (2.7988)\tLoss_giou 61.8755 (61.8278)\tAccu 0.5625 (0.6984)\tMean_iu 0.1136 (0.1176)\t\n",
      "Epoch: [6][601/1843]\tTime 3.044 (2.954)\tLoss 64.8416 (64.6373)\tLoss_bbox 3.0372 (2.7965)\tLoss_giou 61.8044 (61.8408)\tAccu 1.3438 (0.7144)\tMean_iu 0.0998 (0.1198)\t\n",
      "Epoch: [6][901/1843]\tTime 3.046 (2.987)\tLoss 64.8384 (64.6444)\tLoss_bbox 3.2309 (2.8023)\tLoss_giou 61.6075 (61.8421)\tAccu 0.5625 (0.6979)\tMean_iu 0.0690 (0.1199)\t\n",
      "Epoch: [6][1201/1843]\tTime 3.005 (3.006)\tLoss 64.8385 (64.6497)\tLoss_bbox 3.1833 (2.8088)\tLoss_giou 61.6552 (61.8409)\tAccu 0.4062 (0.6988)\tMean_iu 0.0933 (0.1198)\t\n",
      "Epoch: [6][1501/1843]\tTime 3.006 (3.011)\tLoss 65.3630 (64.6514)\tLoss_bbox 3.3173 (2.8115)\tLoss_giou 62.0457 (61.8399)\tAccu 0.8125 (0.6894)\tMean_iu 0.1621 (0.1195)\t\n",
      "Epoch: [6][1801/1843]\tTime 2.986 (3.012)\tLoss 64.2676 (64.6493)\tLoss_bbox 2.6161 (2.8090)\tLoss_giou 61.6515 (61.8403)\tAccu 0.0000 (0.6797)\tMean_iu 0.0831 (0.1193)\t\n",
      "Validate: [1/370]\tTime 2.600 (2.600)  Acc 1.0938 (1.0938)  Mean_iu 0.1369 (0.1369)  \n",
      "Validate: [101/370]\tTime 2.603 (2.623)  Acc 1.5000 (0.9208)  Mean_iu 0.1316 (0.1422)  \n",
      "Validate: [201/370]\tTime 2.624 (2.624)  Acc 0.4688 (0.9468)  Mean_iu 0.0860 (0.1412)  \n",
      "Validate: [301/370]\tTime 2.722 (2.624)  Acc 0.7812 (0.9516)  Mean_iu 0.1517 (0.1428)  \n",
      "Train_epoch 6  Validate Result:  Acc 0.9269425675675675, MIoU 0.14194350178177292.\n",
      "Epoch: [7][1/1843]\tTime 2.986 (2.986)\tLoss 64.7682 (64.7682)\tLoss_bbox 2.8447 (2.8447)\tLoss_giou 61.9235 (61.9235)\tAccu 1.3750 (1.3750)\tMean_iu 0.1505 (0.1505)\t\n",
      "Epoch: [7][301/1843]\tTime 2.931 (2.992)\tLoss 64.7125 (64.6545)\tLoss_bbox 2.9620 (2.8172)\tLoss_giou 61.7504 (61.8373)\tAccu 0.4375 (0.6746)\tMean_iu 0.0940 (0.1191)\t\n",
      "Epoch: [7][601/1843]\tTime 2.933 (2.987)\tLoss 64.8454 (64.6604)\tLoss_bbox 3.0690 (2.8268)\tLoss_giou 61.7763 (61.8335)\tAccu 0.3438 (0.6697)\tMean_iu 0.1048 (0.1190)\t\n",
      "Epoch: [7][901/1843]\tTime 2.967 (2.980)\tLoss 64.8700 (64.6672)\tLoss_bbox 2.8073 (2.8274)\tLoss_giou 62.0627 (61.8398)\tAccu 0.7500 (0.6702)\tMean_iu 0.1577 (0.1198)\t\n",
      "Epoch: [7][1201/1843]\tTime 2.979 (2.978)\tLoss 65.3248 (64.6605)\tLoss_bbox 3.4635 (2.8213)\tLoss_giou 61.8612 (61.8392)\tAccu 0.5625 (0.6704)\tMean_iu 0.1256 (0.1194)\t\n",
      "Epoch: [7][1501/1843]\tTime 2.906 (2.977)\tLoss 64.5139 (64.6587)\tLoss_bbox 2.7198 (2.8199)\tLoss_giou 61.7941 (61.8388)\tAccu 0.4375 (0.6685)\tMean_iu 0.1120 (0.1190)\t\n",
      "Epoch: [7][1801/1843]\tTime 2.946 (2.972)\tLoss 64.4748 (64.6523)\tLoss_bbox 2.5389 (2.8114)\tLoss_giou 61.9359 (61.8409)\tAccu 1.1875 (0.6837)\tMean_iu 0.1297 (0.1193)\t\n",
      "Validate: [1/370]\tTime 2.608 (2.608)  Acc 0.5625 (0.5625)  Mean_iu 0.1318 (0.1318)  \n",
      "Validate: [101/370]\tTime 2.599 (2.623)  Acc 0.6250 (0.9298)  Mean_iu 0.1475 (0.1376)  \n",
      "Validate: [201/370]\tTime 2.609 (2.628)  Acc 0.8438 (0.9439)  Mean_iu 0.1063 (0.1377)  \n",
      "Validate: [301/370]\tTime 2.603 (2.624)  Acc 0.5625 (0.9172)  Mean_iu 0.1393 (0.1361)  \n",
      "Train_epoch 7  Validate Result:  Acc 0.9111486486486486, MIoU 0.13525474158090514.\n",
      "Best Acc: 1.2269425675675676.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "epochs = 7\n",
    "hidden_dim = 32\n",
    "img_emb_dim = 768\n",
    "word_emb_dim = 768\n",
    "num_heads = 8\n",
    "trx_ff_dim = 50\n",
    "bs = 32\n",
    "model = MainModel(img_emb_dim, word_emb_dim, num_heads, trx_ff_dim\n",
    "                 ,hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10e-4, weight_decay=10e-3)\n",
    "\n",
    "# get criterion\n",
    "criterion = Criterion()\n",
    "best_accu = -float('Inf')\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_epoch(train_loader, model, optimizer, epoch,criterion, 512)\n",
    "    model.eval()\n",
    "    accu_new, miou_new = validate_epoch(val_loader, model, epoch, 512)\n",
    "\n",
    "    is_best = accu_new > best_accu\n",
    "    best_accu = max(accu_new, best_accu)\n",
    "    \n",
    "\n",
    "print(f'Best Acc: {best_accu}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:35:25.390174Z",
     "iopub.status.busy": "2023-04-25T14:35:25.389692Z",
     "iopub.status.idle": "2023-04-25T14:35:25.468360Z",
     "shell.execute_reply": "2023-04-25T14:35:25.466815Z",
     "shell.execute_reply.started": "2023-04-25T14:35:25.390121Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/kaggle/working/model_7ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:35:25.470992Z",
     "iopub.status.busy": "2023-04-25T14:35:25.470505Z",
     "iopub.status.idle": "2023-04-25T14:51:22.111682Z",
     "shell.execute_reply": "2023-04-25T14:51:22.110576Z",
     "shell.execute_reply.started": "2023-04-25T14:35:25.470942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:  Acc 0.9088255494505495, MIoU 0.1353150097100617.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_epoch(test_loader, model,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
